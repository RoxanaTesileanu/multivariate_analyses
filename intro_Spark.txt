Reference: "Learning Spark" 2015
https://spark.apache.org/docs/latest/programming-guide.html
http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html#combineByKey
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.package


Initialize your SparkContext in Scala:

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
// local means that Spark runs on the local machine, without connecting to a cluster
// app name is for the case you connect to a cluster (this will the cluster manager's UI to identify your app


val sc = new SparkContext(conf)

To shut down Spark, you can call the stop() method on your SparkContext.

Example: Word Counting of the README.md file in the DeepLearning directory

1. initialize your context (val conf and val sc)
2. val input = sc.textFile("README.md") // load the input data
3. val words = input.flatMap(line => line.split(" ")) // flatMap transforms the input according to the function and flattens it to a list

4. val counts = words.map(word => (word, 1)).reduceByKey{case(x,y) => x+y}
// after numbering the words within a pair you reduce the list of key-value pairs.

A few special operations are only available on RDDs of key-value pairs, like e.g. grouping or aggregating the elements by a key.
 
5. counts.saveAsTextFile("spark_output.txt")

____________________________________________________________________

Programming with RDDs

An RDD can contain any type of Python, Java or Scala objects, including user-defined classes, and it can be split into multiple partitions, which may be computed on different nodes of the cluster. 

Users can create an RDD in two ways: by loading an external dataset, or by distributing a collection of objects in their driver program.

RDDs offer two types of operations: transformations and actions.

val pythonLines = input.filter(line => line contains "Python") // a transformation
//pythonLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[22] at filter at <console>:17


Transformations construct a new RDD from a previous one. Actions compute a result based on an RDD and either return it to the driver program or save it to an external storage system. 

pythonLines.first() // an action 
//res6: String = - spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".


!!! RDDs can be defined any time, but Spark computes them in a lazy fashion (they are created the first time they are instantiated, i.e. the first time they are used) !!!
Spark computes just the data needed for its current result. RDDs are recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using:

RDD.persist()

In practice you will use persist() to load a subset of your data into memory and query it repeatedly:

pythonLines.persist()
pythonLines.count() 
// res5: Long = 1
pythonLines.first() 
//res6: String = - spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".

To summarize, every Spark session will:
- create some input RDDs from external data
- perform transformations to create new RDDs using e.g. filter()
- ask Spark to persist() any intermediate RDDs that you will reuse
- launch actions on RDDs like e.g. count()

________________________________________________________

Creating RDDs: 
loading an external data set and parallelizing a collection in your driver program

The simplest way to create RDDs is to take an existing collection in your program and pass it to Spark's Context with the parallelize() method. It requires that you have your entire dataset in memory on one machine.

val myRDD = sc.parallelize(List("I like", "you"))
//myRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23] at parallelize at <console>:17


A more common way to create RDDs is to load data from external storage. 

____________________________________________________________

RDD Operations:
transformations and actions

1) Transformations are operations on RDDs that return a new RDD (map(), filter(), etc.).
Actions are operations that return a result to the driver program or write it to storage.

TRANSFORMATIONS RETURN AN RDD WHEREAS ACTIONS RETURN SOME OTHER DATA TYPE!

val scalaLines = input.filter(line => line contains "Scala")
//scalaLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at filter at <console>:17

val languageLines = pythonLines union scalaLines
//languageLines: org.apache.spark.rdd.RDD[String] = UnionRDD[25] at union at <console>:18

Transformations can operate on any number of input RDDs.

SPARK KEEPS TRACK OF THE SET OF DEPENDENCIES BETWEEN DIFFERENT RDDs USING A LINEAGE GRAPH. This information can be used to recover lost data.

Transformations on RDDs are lazily evaluated.

2) Actions are operations that return a final value to the driver program or write data to an external storage system.


println("The number of lines where the used languages appear is " + languageLines.count())

languageLines.collect().foreach(println)
//- spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".
- implementing neural networks in Scala using "Learning Scala" (Swartz 2015), "Learning Spark" (Karau et al. 2015), "Coursera" (Oderski et al.) https://www.coursera.org/learn/progfun1 

languageLines.take(1)
//res12: Array[String] = Array(- spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".)

Use take() to retrieve a small no. of elements in the RDD. 
Use collect() to retrieve the entire RDD. Your entire dataset must fit in memory on a single machine to use collect().

You can save the contents of an RDD using the saveAsTextFile() action or other ways of exporting data.

myRDD.saveAsTextFile("myRDD.txt")

_____________________________________________________________

Passing functions to Spark in Scala

def manipulatingRDDs (rdd: org.apache.spark.rdd.RDD[Int]) : org.apache.spark.rdd.RDD[Double]= {rdd.map(_ + 0.5)}
//manipulatingRDDs: (rdd: org.apache.spark.rdd.RDD[Int])org.apache.spark.rdd.RDD[Double]


val myRDDcounts = sc.parallelize(List(30,21,11,10,65))
//myRDDcounts: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:17

val transfRDDcounts= manipulatingRDDs(myRDDcounts)
//transfRDDcounts: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[3] at map at <console>:16

transfRDDcounts foreach (println)
//...
17/07/03 15:48:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
30.5
21.5
11.5
10.5
65.5
....


______________________________________________________________

At the end of the day don't forget to stop Spark in a nice manner or else you get weird errors about not being able to handle more than one context, etc.:

sc.stop()
//17/07/03 15:52:01 INFO SparkUI: Stopped Spark web UI at http://192.1 68.1.102:4040
17/07/03 15:52:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/07/03 15:52:01 INFO MemoryStore: MemoryStore cleared
17/07/03 15:52:01 INFO BlockManager: BlockManager stopped
17/07/03 15:52:01 INFO BlockManagerMaster: BlockManagerMaster stopped
17/07/03 15:52:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/07/03 15:52:01 INFO SparkContext: Successfully stopped SparkContext

______________________________________________________________

Common transformations and actions

Additional operations are available on RDDs containing certain types of data. 

Basic operations (on all types):

- elementwise transformations: e.g. map() and filter()

val inputRDD = sc.parallelize(List(1,2,3,4))
val mappedRDD = inputRDD map(x=> x*x)
val filteredRDD = inputRDD filer(x=> x != 1)

! map()'s return type does not have to be the same as its input type !


val result = mappedRDD.collect().mkString(",")
//result: String = 1,4,9,16

To produce multiple outputs from one input element we use flatMap().

val greeting = sc.parallelize(List("Hi, there!"))
val singleChar = greeting.flatMap(c => c.split(""))
singleChar.first()
singleChar.count()

- pseudo set transformations:
RDDs support many of the operations of mathematical sets: distinct(), union(), intersection(), subtract(), even if RDDs themselves are not properly sets. These operations require that the RDDs being operated on are of the same type.

val RDD1 = sc.parallelize(List("coffee", "coffee", "panda", "monkey", "tea"))
val RDD2= sc.parallelize(List("coffee", "monkey", "kitty"))

RDD1 distinct() collect() foreach(println)
RDD1 union RDD2 collect() foreach(println) // might contain duplicates
RDD1 intersection RDD2 collect() foreach(println)
RDD1 subtract RDD2 collect() foreach(println) // returns what has RDD1 and doesn't have RDD2

- actions: the most common action used is reduce()
With reduce() we can easily sum the elements of the RDD, count the no. of elements and perform other types of aggregations.

val sum = inputRDD reduce((x,y) => x+y)

The aggregate()() function:
using the info found at: http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html

val result = inputRDD.aggregate(0)(_ + _, _ + _)
// returns the same as val sum of course, but you can use the aggregate function to use two different operations not the same.

If you use a Tuple2 in the first parameter list you start with two accumulator values. Let's suppose we have both set to 0. Then, suppose we want the first acc to actually do the summing, and the second one to count the elements which have been summed. At the end we have to add the informations for both elements of the tuple to compute the average! 

val result = inputRDD.aggregate((0,0))(
	(acc, value) => (acc._1 + value, acc._2 +1),
	(acc1, acc2) => (acc1._1 +acc2._1, acc1._2 + acc2._2))
val avg = result._1/result._2.toDouble

More general the oversimplified syntax for aggregate is: 

aggregate(acc)(func1, func2)


The simplest and most common operation that returns data to our driver program is collect(), which returns the entire RDD's contents. If there is an ordering defined on our data, we can also extract the top elements from an RDD using top(). It uses the default ordering of the data, but we can supply our own comparison function to extract the top elements. The foreach() action lets us perform computations on each element in the RDD without bringing it back locally. The countByValue() method returns a map of each unique value to its count.

Some functions are available only on certain types of RDDs, such as mean() and variance() on numeric RDDs and join() on key/value pair RDDs. 

________________________________________________________

Persistence

Spark's RDDs are lazily evaluated and sometimes we wish to use the same RDD multiple times. To avoid computing the same RDD multiple times (esp. for iterative algorithms), we can ask Spark to persist the data.

Example: double execution 

val mappedRDD2= inputRDD.map(x=> x +3)
println(mappedRDD2.count())
println(mappedRDD2.collect().mkString(","))

To avoid computing an RDD multiple times, we can ask Spark to persist the data. 
Spark has many levels of persistence to choose from based on what our goals are. 
In Scala the default persist() will store the data in the JVM heap as unserialized objects. 
When we write data out to disk or off-heap storage, that data is always serialized.

Persistence levels from org.apache.spark.storage.StorageLevel:
MEMORY_ONLY (space used high, CPU time low)
MEMORY_ONLY_SER (space used low, CPU time high)
MEMORY_AND_DISK (space used high, CPU time medium)
MEMORY_AND_DISK_SER (space used low, CPU time high)
DISK_ONLY (space used low, CPU time high)

Example: persist() in Scala
val mappedRDD2= inputRDD.map(x=> x +3)
mappedRDD2.persist(StorageLevel.DISK_ONLY)
println(mappedRDD2.count())
println(mappedRDD2.collect().mkString(","))

RDDs also come with a method unpersist() that lets you remove them from cache.

_______________________________________________________________

Working with key/value pairs

Key/value RDDs are commonly used to perform aggregations.
Spark provides special operations on RDDs containing key/value pairs (pair RDDs).

Creating pair RDDs:

We can transform a regular RDD into a pair RDD by running a map() function that returns key/value pairs. Or, you can call SparkContext.parallelize() on a collection of pairs.

val pairsRDD = inputRDD.map(x=> ((0),x))
//pairsRDD: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[65] at map at <console>:17

val pairsRDD2 = inputRDD.map(x=> ('a',x))
//pairsRDD2: org.apache.spark.rdd.RDD[(Char, Int)] = MapPartitionsRDD[66] at map at <console>:17

val pairsRDD3= myRDD.zipWithIndex
//pairsRDD3: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[74] at zipWithIndex at <console>:17

pairsRDD3.collect().foreach(println)
//...
(I like,0)
(you,1)
(very much,2)
....

val pairsRDD4 = sc.parallelize(List(('a',1), ('b',2),('c',3), ('d',4)))


Transformations on pair RDDs:


Let's define two RDDs to work with

val favRDD = sc.parallelize(List("pizza", "spagetti", "pizza","mamaliga", "biscuiti", "mamaliga"))
//favRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:17

val orderFavRDD = sc.parallelize(List(4,2,3,4,1,9))
orderFavRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:17

val combFavRDD = favRDD zip orderFavRDD
combFavRDD: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[4] at zip at <console>:18

combFavRDD collect() foreach(println)
//...
(pizza,4)
(spagetti,2)
(pizza,3)
(mamaliga,4)
(biscuiti,1)
(mamaliga,9)

val redByKey = combFavRDD.reduceByKey((x,y) => x+y) //combines values with the same key 
//redByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:17

redByKey collect() foreach(println)
//...
(spagetti,2)
(pizza,7)
(mamaliga,13)
(biscuiti,1)


combFavRDD.groupByKey() collect() foreach(println)  //groups values with the same key
//...
(spagetti,CompactBuffer(2))
(pizza,CompactBuffer(4, 3))
(mamaliga,CompactBuffer(4, 9))
(biscuiti,CompactBuffer(1))

Now, regarding the combineByKey() method. I have to take a closer look at it.
From RDD API:
def combineByKey[C](createCombiner: (V) ⇒ C, mergeValue: (C, V) ⇒ C, mergeCombiners: (C, C) ⇒ C): RDD[(K, C)]

It seams that the oversimplified structure is: combineByKey(func1, func2, func3):RDD[(K,C)].
I want to combine the values smaller than 3 into a list:

combFavRDD.combineByKey(
     | (v: Int) => ((v<=3).toList, (v >3).toList),
     | (groups: (List[String],List[String]), v)=> ((groups._1, v), (groups._2, v)),
     | (group1: (String,Int), group2: (String,Int)) => List(group1, group2))
//error

Ok, I've realized this function is actually a key aggregator by default and you can leave out the "String part" completely. So in finding the values smaller than 3 I probably should use something more "value-oriented" instead of "key-oriented"...
Now, if I want to be key-oriented, I could for example sum the orders for each key:
    
val resultComByKey = combFavRDD.combineByKey(
     | (count) => (count),
     | (acc : (Int), v) => (acc +v),
     | (acc1:(Int), acc2:(Int)) => (acc1 + acc2))
//resultComByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at combineByKey at <console>:17

resultComByKey collect
//...
res22: Array[(String, Int)] = Array((spagetti,2), (pizza,7), (mamaliga,13), (biscuiti,1))

It is the same result as reduceByKey() but I've made it to undestand the mechanics of it! I could use a more complex combiner to follow the example from the book at p. 54.  

Example: per-key average using combineByKey() in Scala

val resultComByKey2 = combFavRDD.combineByKey(
     | (v) => (v,1),
     | (acc : (Int,Int), v) => (acc._1 + v, acc._2 +1),
     | (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
//resultComByKey2: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[19] at combineByKey at <console>:17

resultComByKey2 collect()
//res24: Array[(String, (Int, Int))] = Array((spagetti,(2,1)), (pizza,(7,2)), (mamaliga,(13,2)), (biscuiti,(1,1)))

val keyAvr = resultComByKey2.map{
     | case (key, value) => (key, value._1/value._2.toFloat)}
//keyAvr: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[21] at map at <console>:17

keyAvr collect()
//res26: Array[(String, Float)] = Array((spagetti,2.0), (pizza,3.5), (mamaliga,6.5), (biscuiti,1.0))

val finalResult = keyAvr collectAsMap()
//finalResult: scala.collection.Map[String,Float] = Map(spagetti -> 2.0, biscuiti -> 1.0, mamaliga -> 6.5, pizza -> 3.5)

So, after collecting the RDD as Map, I've made it to come to a Scala Map at the end!




 



 







