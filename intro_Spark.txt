Reference: "Learning Spark" 2015
https://spark.apache.org/docs/latest/programming-guide.html

Initialize your SparkContext in Scala:

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
// local means that Spark runs on the local machine, without connecting to a cluster
// app name is for the case you connect to a cluster (this will the cluster manager's UI to identify your app


val sc = new SparkContext(conf)

To shut down Spark, you can call the stop() method on your SparkContext.

Example: Word Counting of the README.md file in the DeepLearning directory

1. initialize your context (val conf and val sc)
2. val input = sc.textFile("README.md") // load the input data
3. val words = input.flatMap(line => line.split(" ")) // flatMap transforms the input according to the function and flattens it to a list

4. val counts = words.map(word => (word, 1)).reduceByKey{case(x,y) => x+y}
// after numbering the words within a pair you reduce the list of key-value pairs.

A few special operations are only available on RDDs of key-value pairs, like e.g. grouping or aggregating the elements by a key.
 
5. counts.saveAsTextFile("spark_output.txt")

____________________________________________________________________

Programming with RDDs

An RDD can contain any type of Python, Java or Scala objects, including user-defined classes, and it can be split into multiple partitions, which may be computed on different nodes of the cluster. 

Users can create an RDD in two ways: by loading an external dataset, or by distributing a collection of objects in their driver program.

RDDs offer two types of operations: transformations and actions.

val pythonLines = input.filter(line => line contains "Python") // a transformation
//pythonLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[22] at filter at <console>:17


Transformations construct a new RDD from a previous one. Actions compute a result based on an RDD and either return it to the driver program or save it to an external storage system. 

pythonLines.first() // an action 
//res6: String = - spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".


!!! RDDs can be defined any time, but Spark computes them in a lazy fashion (they are created the first time they are instantiated, i.e. the first time they are used) !!!
Spark computes just the data needed for its current result. RDDs are recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using:

RDD.persist()

In practice you will use persist() to load a subset of your data into memory and query it repeatedly:

pythonLines.persist()
pythonLines.count() 
// res5: Long = 1
pythonLines.first() 
//res6: String = - spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".

To summarize, every Spark session will:
- create some input RDDs from external data
- perform transformations to create new RDDs using e.g. filter()
- ask Spark to persist() and intermediate RDDs that you will reuse
- lauch actions on RDDs like e.g. count()

________________________________________________________

Creating RDDs: 
loading an external data set and parallelizing a collection in your driver program

The simplest way to create RDDs is to take an existing collection in your program and pass it to Spark's Context with the parallelize() method. It requires that you have your entire dataset in memory on one machine.

val myRDD = sc.parallelize(List("I like", "you"))
//myRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23] at parallelize at <console>:17


A more common way to create RDDs is to load data from external storage. 

____________________________________________________________

RDD Operations:
transformations and actions

1) Transformations are operations on RDDs that return a new RDD (map(), filter(), etc.).
Actions are operations that return a result to the driver program or write it to storage.

TRANSFORMATIONS RETURN AN RDD WHEREAS ACTIONS RETURN SOME OTHER DATA TYPE!

val scalaLines = input.filter(line => line contains "Scala")
//scalaLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at filter at <console>:17

val languageLines = pythonLines union scalaLines
//languageLines: org.apache.spark.rdd.RDD[String] = UnionRDD[25] at union at <console>:18

Transformations can operate on any number of input RDDs.

SPARK KEEPS TRACK OF THE SET OF DEPENDENCIES BETWEEN DIFFERENT RDDs USING A LINEAGE GRAPH. This information can be used to recover lost data.

Transformations on RDDs are lazily evaluated.

2) Actions are operations that return a final value to the driver program or write data to an external storage system.


println("The number of lines where the used languages appear is " + languageLines.count())

languageLines.collect().foreach(println)
//- spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".
- implementing neural networks in Scala using "Learning Scala" (Swartz 2015), "Learning Spark" (Karau et al. 2015), "Coursera" (Oderski et al.) https://www.coursera.org/learn/progfun1 

languageLines.take(1)
//res12: Array[String] = Array(- spatial data analysis with R from Bivand, Pebesma and Gomez-Rubio 2008 "Spatial data analysis with R" and Chris Garrard 2016 "Geoprocessing with Python".)

Use take() to retrieve a small no. of elements in the RDD. 
Use collect() to retrieve the entire RDD. Your entire dataset must fit in memory on a single machine to use collect().

You can save the contents of an RDD using the saveAsTextFile() action or other ways of exporting data.

myRDD.saveAsTextFile("myRDD.txt")

_____________________________________________________________

Passing functions to Spark in Scala

def manipulatingRDDs (rdd: org.apache.spark.rdd.RDD[Int]) : org.apache.spark.rdd.RDD[Double]= {rdd.map(_ + 0.5)}
//manipulatingRDDs: (rdd: org.apache.spark.rdd.RDD[Int])org.apache.spark.rdd.RDD[Double]


val myRDDcounts = sc.parallelize(List(30,21,11,10,65))
//myRDDcounts: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:17

val transfRDDcounts= manipulatingRDDs(myRDDcounts)
//transfRDDcounts: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[3] at map at <console>:16

transfRDDcounts foreach (println)

//...
17/07/03 15:48:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
30.5
21.5
11.5
10.5
65.5
....


______________________________________________________________

At the end of the day don't forget to stop Spark in a nice manner or else you get weird errors about not being able to handle more than one context, etc.:

sc.stop()
//17/07/03 15:52:01 INFO SparkUI: Stopped Spark web UI at http://192.1 68.1.102:4040
17/07/03 15:52:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/07/03 15:52:01 INFO MemoryStore: MemoryStore cleared
17/07/03 15:52:01 INFO BlockManager: BlockManager stopped
17/07/03 15:52:01 INFO BlockManagerMaster: BlockManagerMaster stopped
17/07/03 15:52:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/07/03 15:52:01 INFO SparkContext: Successfully stopped SparkContext

______________________________________________________________



























   






