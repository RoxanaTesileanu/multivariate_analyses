Reference: "Learning Spark" 2015
https://spark.apache.org/docs/latest/programming-guide.html

Initialize your SparkContext in Scala:

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
// local means that Spark runs on the local machine, without connecting to a cluster
// app name is for the case you connect to a cluster (this will the cluster manager's UI to identify your app


val sc = new SparkContext(conf)

To shut down Spark, you can call the stop() method on your SparkContext.

Example: Word Counting of the README.md file in the DeepLearning directory

1. initialize your context (val conf and val sc)
2. val input = sc.textFile("README.md") // load the input data
3. val words = input.flatMap(line => line.split(" ")) // flatMap transforms the input according to the function and flattens it to a list

4. val counts = words.map(word => (word, 1)).reduceByKey{case(x,y) => x+y}
// after numbering the words within a pair you reduce the list of key-value pairs.

A few special operations are only available on RDDs of key-value pairs, like e.g. grouping or aggregating the elements by a key.
 
5. counts.saveAsTextFile("spark_output.txt")

____________________________________________________________________

Programming with RDDs

An RDD can contain any type of Python, Java or Scala objects, including user-defined classes, and it can be split into multiple partitions, which may be computed on different nodes of the cluster. 

Users can create an RDD in two ways: by loading an external dataset, or by distributing a collection of objects in their driver program.

RDDs offer two types of operations: transformations and actions.

val pythonLines = input.filter(line => line contains "Python") // a transformation

Transformations construct a new RDD from a previous one. Actions compute a result based on an RDD and either return it to the driver program or save it to an external storage system. 

pythonLines.first() // an action 

!!! RDDs can be defined any time, but Spark computes them in a lazy fashion (they are created the first time they are instantiated, i.e. the first time they are used) !!!
Spark computes just the data needed for its current result. RDDs are recomputed each time you run an action on them. If you would like 






   






