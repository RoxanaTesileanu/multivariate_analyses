Discriminant Adaptive Nearest Neighbor Classification and Regression 1996

- locally adaptive form of nearest neighbors classification for the case when class conditional probabilities are not locally constant
- use a local LDA to estimate an effective metric for computing neighborhoods
- propose a method for global dimension reduction that combines local dimension information
- the classification problem: J classes, N training observations.
The training observations consist of predictor measurements x={x1, x2, ..., xp) on p predictors and the known class memberships for the training data. 
The goal is to predict the class membership of an observation with predictor vector x0.

Nearest neighbor classification:
- we find the set of K nearest neighbors in the training set of x0 and then classify x0 as the most frequent class among the K neighbors. 

Discriminant adaptive NN:
- let B and W denote the between and within sum of squares matrices.
- we determine the local decision boundaries from centroid information
- from MANOVA information: we take the linear combination producing the largest eigenvalue (i.e. the one which maximizes the explained variance between groups) 

________________________________________________________________

From Hastie, Tibshirani, Friedman (2009, The Elements of Statistical Learning)
- in the case of mixtures of tightly clustered Gaussians, a linear decision boudary is unlikely to be optimal => optimal decision boudary is nonlinear and disjoint => k- NN methods are better suited for such a scenario (p.14).

- NN methods use the observations in the training set T closest in input space to x to form Y-hat. 
- the k NN fit for Y-hat is defined as: Y-hat(x) = 1/k sum(yi).
- xi belongs to Nk(x). Nk(x) is the neihborhood of x defined by the k closest points xi in the training sample.
- closeness implies a metric which we assume to be the Euclidean distance
Any particular subregion of the decision boundary depends on a handful of input points and their particular positions (high variance and low bias of the decision boundary). 
______________________________________________________________

This is an observation-based approach (distances between observations).
MANOVA is a variable-based approach (variance of variables). 
In high-dimensional spaces it could be helpful to combine the two approaces. This is what Hastie and Tibshirani do in combining the LDA and NN in DANN.

 
 


  


