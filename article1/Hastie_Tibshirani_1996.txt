Discriminant Adaptive Nearest Neighbor Classification and Regression 1996

- locally adaptive form of nearest neighbors classification for the case when class conditional probabilities are not locally constant
- use a local LDA to estimate an effective metric for computing neighborhoods
- propose a method for global dimension reduction that combines local dimension information
- the classification problem: J classes, N training observations.
The training observations consist of predictor measurements x={x1, x2, ..., xp) on p predictors and the known class memberships for the training data. 
The goal is to predict the class membership of an observation with predictor vector x0.

Nearest neighbor classification:
- we find the set of K nearest neighbors in the training set of x0 and then classify x0 as the most frequent class among the K neighbors. 

Discriminant adaptive NN:
- let B and W denote the between and within sum of squares matrices.
- we determine the local decision boundaries from centroid information
- from MANOVA information: we take the linear combination producing the largest eigenvalue (i.e. the one which maximizes the explained variance between groups) 

________________________________________________________________

From Hastie, Tibshirani, Friedman (2009, The Elements of Statistical Learning)
- in the case of mixtures of tightly clustered Gaussians, a linear decision boudary is unlikely to be optimal => optimal decision boudary is nonlinear and disjoint => k- NN methods are better suited for such a scenario (p.14).

- NN methods use the observations in the training set T closest in input space to x to form Y-hat. 
- the k NN fit for Y-hat is defined as: Y-hat(x) = 1/k sum(yi).
- xi belongs to Nk(x). Nk(x) is the neihborhood of x defined by the k closest points xi in the training sample.
- closeness implies a metric which we assume to be the Euclidean distance
Any particular subregion of the decision boundary depends on a handful of input points and their particular positions (high variance and low bias of the decision boundary). 
______________________________________________________________

This is an observation-based approach (distances between observations).
MANOVA is a variable-based approach (variance of variables). 
In high-dimensional spaces it could be helpful to combine the two approaches. This is what Hastie and Tibshirani do in combining the LDA and NN in DANN.
They find local subspaces where linear approaches apply.
______________________________________________________________


Now, if the dataset passes the MANOVA test of differences in group centroids, there is a linear combination which can discriminate between groups globally. But, in the zones where the groups are overlapping, if the groups are clustered tightly, the predictions might be false. This is where the predicted class is rathermore correctly assigned by the majority vote of NN methods (for cases where the majority vote is correct and the prediction from the linear combination is false).

From the error cases from the linear combination obtained in MANOVA (which represent the residual), you delineate spaces for kNN application. In the machine-learning pipeline you can vary the no. of k and see how the error rate behaves for each space. 

_______________________________________________________________

