\documentclass[twoside, 11p]{article}

\usepackage{jmlr2e}
\usepackage{cite} %bibliographystyle plannat

\begin{document}
\title{Introduction to Statistical Computing in Scala - an Implementation of the K-Nearest Neighbors classifier}

\author{\name Roxana Tesileanu \email roxana.te@web.de \\
	\addr Environmental Statistics and Bioinformatics \\
	National Institute of Forest Research and Management (INCDS)\\
	Brasov, Romania} 
\editor{Kevin Murphy and Bernhard Sch{\"o}lkopf}

\maketitle

\begin{abstract}%
 Statistical computing in ecology evolves at a high speed, mainly because researchers have recognized the advantage of being able to design their algorithms according to their needs.
 The present paper introduces the implementation in Scala language of the k-Nearest Neighbors (kNN) classifier, which can be applied also on small datasets, a situation commonly encountered in ecological research, and discusses its possible role within a more complex learning pipeline for classification tasks. 
\end{abstract}

\begin{keywords}
learning pipeline for classification, machine learning in ecology, diffusion processes,  k-Nearest Neighbors classification, Scala language, Simple Build Tool (SBT), multivariate analyses 
\end{keywords}

\section{Introduction}
   
One of the drivers of the machine-learning progress is the great amount of data born within and gathered by networked and mobile computing systems which necessitates further processing in order to gain insights into the specific fields from which it originates \cite{jordan_machine_2015}. 
The "Big Data phenomenon" is real but, unfortunately, it can not be generalized to all research areas, especially some areas of ecological research which investigate systems which are by nature data-poor and will probably remain as such unless cost-intensive data collection projects are being proposed and financed. This by no means implies that such areas cannot take advantage of the progress of machine-learning and take the best out of the existing datasets.        
On the contrary, machine-learning, being a study field which "sits at the crossroads of computer science, statistics and a variety of other disciplines concerned with automatic improvement over time, and inference and decision-making under uncertainty" \cite{jordan_machine_2015}, is welcomed in real-world environmental decision-making facing the need of proposing courses of action under the cloud of uncertainties and regarding issues characterized by multiple attributes \cite{vatn_choices_1994}.   
Moreover, machine-learning is used not only in analyzing data from observational studies (which might benefit from larger time series datasets), but also from experimental studies (which can't produce large amounts of data but benefit from an experimental design, delivering data which provides stronger inference about effects and causal relationships \cite{quinn_experimental_2002}) \cite{harrington_machine_2012}.
Indeed, a combination of machine-learning classifiers with the benefits of a MANOVA experimental design used for collecting training data, might produce useful research results, in ecological research. Thus, in order to be able to use the power of machine-learning algorithms adapted to small ecological datasets, a statistical package written in Scala is currently under development at INCDS.\\


The idea of adapting the use of machine-learning algorithms to real-world data requirements is not new. 
Hastie and Tibshirani \cite{hastie_discriminant_nodate}, describe the Discriminant Adaptive Nearest Neighbor (DANN) procedure as a locally adaptive form of nearest neighbors classification using a modified linear discriminant analysis (LDA) procedure to estimate an effective metric for computing neighborhoods.
The main idea of combining distance-based approaches with approaches based on the association between variables is driven by the fact that a multivariate dataset can be analysed in both ways \cite{quinn_experimental_2002}: calculating scores for the derived variables (components) for each object, and calculating dissimilarity measures (distances) for every possible pair of objects.\\

      
The present paper introduces the Scala functions used to run the kNN classification algorithm and then it delineates its role within a real-world classification pipeline for reducing error.    
 


