References:

Bugnion Pascal 2016 - "Scala for Data Science", Packt Publishing, Birmingham 
Harrington Peter 2012 - "Machine learning in action", Manning Publications Co., Shelter Island
Karau Holden, Konwinski Andy, Wendell Patrick, Zaharia Matei 2015 - "Learning Spark", O'Reilly, Sebastopol   
Nicolas R. Patrick  2015 - "Scala for Machine Learning", Packt Publishing, Birmingham 
Swartz Jason 2015 - "Learning Scala", Manning Publications Co., Shelter Island 
Trask Andrew 2017 - "Grokking deep learning", Manning Publications Co., Shelter Island 
Odersky Martin, Spoon Lex, Venners Bill 2010 - "Programming in Scala", Second Edition, Artima, Walnut Creek
Wilkinson Darren 2017 - "Statistical Computing with Scala: A functional approach to data science" -  https://github.com/darrenjw/scala-course
http://technobium.com/logistic-regression-using-apache-spark/

Key terminology

- train the algorithm, i.e. allow it to learn, using a training set
- the training set contains training instances with features (i.e. explanatory variables) and one or more target variables (i.e. response variables)

ch.1 Getting started

ML problems are categorized as classifications, prediction, optimization and regression.
Classification - example: identify a disease from a set of symptoms
Prediction - once the model is trained and validated, it can be used to predict some outcomes (a doctor collects symptoms and anticipates the health state).
Optimization - ML techniques increase the chances that the optimization method converges towards a solution (intelligent search) (example: fighting the spread of a new virus requires optimizing a process that may evolve over time as more symptoms and cases are uncovered).
Regression: is a classification technique particularly suitable for a continuous model.

Why Scala? 
- abstraction: monads (derived from the category and group theory to create high-level abstractions); covariant, contravariant functors and bifunctors (related to manifold and vector bundles used in non-linear model building).    

- higher-kind projection: classification problems rely on the estimation of the similarity between vectors of observations. One technique consists in comparing two vectors by computing the normalized inner product. A co-vector alpha is defined as a linear map of a vector to the inner product. 

- covariant functors for vectors: F[U => V] := F[U] => F[V]
- contravariant functors for co-vector: F[U => V] := F[V] => F[U]
- monads: extend the concept of a functor to allow the composition known as the monadic composition of morphisms on a single type.

- scalability: parallelization and chaining of data processing functions using higher-order methods. So one can build scalable, distributed and concurrent applications (using Akka or Apache Spark). 

Model categorization
A model can be predictive, descriptive or adaptive.

- predictive models: discover patterns in the data and extract fundamental trends and relationships between factors (or features). They are used to predict and classify future events or observations. Predictive models are created through supervised learning using a training set.

- descriptive models: attempt to find unusual patterns or affinities in data by grouping observations into clusters with similar properties. They are generated through unsupervised learning.

- adaptive models: created through reinforcement learning (one or more decision-making agents that recommend and possibly execute actions in the attempt of solving a problem, optimizing an objective function, or resolving constraints.

Taxonomy of machine learning algorithms

data mining -> identifying patterns
machine learning -> indentifying and validating models to optimize a performance criterion using data.

A) Unsupervised learning
The goal of unsupervised learning is to discover patterns of regularities and irregularities in a set of observations. There are two categories: discovery of data clusters and discovery of latent factors.
Unsupervised learning does not require labeled data and therefore is easy to implement and execute because no expertise is needed to validate an output. 

A1) Clustering
 
The purpose of data clustering is to partition a collection of data into a number of clusters or data segments. Practically, a clustering algorithm is used to organize observations into clusters, by minimizing the distance between obs. within a cluster and maximizing the distance between obs. across clusters. 

A2) Dimension reduction

Dimension reduction aims at finding the smallest but most relevant set of features needed to build a reliable model. 

B) Supervised learning

There are two categories of supervised learning algorithms:
- generative models
- discriminative models

B1) Generative models
Generative models attempt to fit a joint probability, p(X,Y), of two events (or random variables) X and Y, representing two sets of observed and hidden x and y variables. 
Generative models are commonly introduced through Bayes' rule.

B2) Disciminative models

Contrary to generative models, discriminative models compute the conditional probability p(Y|X) directly, using the same algorithm for training and classification. 


C) Reinforcement learning

Reinforcement learning algorithms output a set of recommended actions for the adaptive system to execute. In its simplest form, this type of algorithms estimate the best course of action. The challenge is the recommended action or policy may depend on partially observable states.

___________________________________

For mathematics, linear algebra and optimization there are some well-established libraries to use, e.g. Apache Commons Math. 

Add the dependency to your build.sbt file:
// https://mvnrepository.com/artifact/org.apache.commons/commons-math3
libraryDependencies += "org.apache.commons" % "commons-math3" % "3.6"

In Commons Math you can find:
- functions, differentiation, and integral and ordinary differential equations
- statistics distributions
- linear and non-linear optimization
- dense and sparse vectors and matrices
- curve fitting, correlation and regression


For plotting you can use the JFreeChart library. Add the dependency to your build.sbt file:
// https://mvnrepository.com/artifact/org.jfree/jfreechart
libraryDependencies += "org.jfree" % "jfreechart" % "1.0.18"

______________________________ 
  
An overview of computational workflows to perform a runtime processing of a dataset:

1. Loading the dataset from files, databases, or any streaming devices
2. Splitting the data set for parallel data processing
3. Preprocessing data 
4. Applying the model to classify new data
5. Assessing the quality of the model

A similar sequence of tasks is used to extract a model from a training dataset:

1. Loading the dataset from files, databases, or any streaming devices
2. Splitting the data set for parallel data processing
3. Preprocessing data 
4. Selecting the training, testing and validation set from the input data
5. Extracting key features (extracting the model: supervised or unsupervised)
6. Validating the model and tuning the model 
7. Storing the model in a file or database so that it can be applied to future obs.

The first phase consists of extracting the model through clustering or training of a supervised learning algorithm. The model is then validated against test data for which the source is the same as the training set but with different observations. Once the model is created and validated, it can be used to classify real-time data or predict future behaviour. Real-world workflows are more complex and require dynamic configuration to allow experimentation of different models. Several alternative classifiers and different filtering algorithms are applied against input data, depending on the latent noise in the raw data.

Writing a simple workflow:
For the first example a simplified version of the binomial logistic regression as the chosen classifier is used as the stock-price-volume action is treated as a continuous or pseudo-continuous process.

Step 1: Scoping the problem

The dataset is in CSV format. The aim is to model the stock price using its daily volume and volatility. We will consider two variables: price and volume.

Step 2: Loading data

val src = scala.io.Source.fromFile("CSCO.csv")
//res0: scala.io.BufferedSource = non-empty iterator
 
val data = src.getLines.map(_.split(",")).toArray.drop(1)
//data: Array[Array[String]] = Array(Array(11/29/2013, 21.39, 21.44, 21.2, 21.25, 25043700, 21.25), Array(11/27/2013, 21.29, 21.35, 21.15, 21.27, 40019800, 21.27), Array(11/26/2013, 21.3, 21.41, 21.08, 21.21, 48612300, 21.21), Array(11/25/2013, 21.45, 21.52, 21.27, 21.27, 37566000, 21.27), Array(11/22/2013, 21.41, 21.5, 21.31, 21.46, 41091400, 21.46), Array(11/21/2013, 21.36, 21.49, 21.25, 21.47, 40695500, 21.47), Array(11/20/2013, 21.47, 21.47, 21.22, 21.23, 44299700, 21.23), Array(11/19/2013, 21.31, 21.54, 21.12, 21.42, 56434700, 21.42), Array(11/18/2013, 21.6, 21.72, 21.2, 21.29, 65955400, 21.29), Array(11/15/2013, 21.46, 21.69, 21.26, 21.54, 85486800, 21.54), Array(11/14/2013, 20.94, 21.44, 20.77, 21.37, 243064300, 21.37), Array(11/13/2013, 23.6, 24, 23.51, 24, 73328400, 24), Array(11...

data(0)
//res8: Array[String] = Array(11/29/2013, 21.39, 21.44, 21.2, 21.25, 25043700, 21.25)
// date, open, high, low, close, volume, adj_close

val data2 = data.map(x => List(x(2), x(3), x(5)))
//data2: Array[List[String]] = Array(List(21.44, 21.2, 25043700), List(21.35, 21.15, 40019800), List(21.41, 21.08, 48612300), List(21.52, 21.27, 37566000), List(21.5, 21.31, 41091400), List(21.49, 21.25, 40695500), List(21.47, 21.22, 44299700), List(21.54, 21.12, 56434700), List(21.72, 21.2, 65955400), List(21.69, 21.26, 85486800), List(21.44, 20.77, 243064300), List(24, 23.51, 73328400), List(23.84, 23.43, 38020200), List(23.58, 23.4, 22694900), List(23.52, 23.09, 30884100), List(23.56, 23.07, 35502900), List(23.37, 23.02, 48142600), List(23.25, 22.42, 46327100), List(22.64, 22.42, 28606700), List(22.68, 22.4, 33328700), List(22.71, 22.44, 34497400), List(22.94, 22.66, 30465600), List(22.93, 22.56, 30074200), List(22.55, 22.31, 24072100), List(22.63, 22.35, 34253400), List(22.43, 22.1, 5...
// get only the needed variables

val data3=data2.map{ case(List(param1, param2, param3))  => List(param1.toDouble, param2.toDouble, param3.toDouble)}
//data3: Array[List[Double]] = Array(List(21.44, 21.2, 2.50437E7), List(21.35, 21.15, 4.00198E7), List(21.41, 21.08, 4.86123E7), List(21.52, 21.27, 3.7566E7), List(21.5, 21.31, 4.10914E7), List(21.49, 21.25, 4.06955E7), List(21.47, 21.22, 4.42997E7), List(21.54, 21.12, 5.64347E7), List(21.72, 21.2, 6.59554E7), List(21.69, 21.26, 8.54868E7), List(21.44, 20.77, 2.430643E8), List(24.0, 23.51, 7.33284E7), List(23.84, 23.43, 3.80202E7), List(23.58, 23.4, 2.26949E7), List(23.52, 23.09, 3.08841E7), List(23.56, 23.07, 3.55029E7), List(23.37, 23.02, 4.81426E7), List(23.25, 22.42, 4.63271E7), List(22.64, 22.42, 2.86067E7), List(22.68, 22.4, 3.33287E7), List(22.71, 22.44, 3.44974E7), List(22.94, 22.66, 3.04656E7), List(22.93, 22.56, 3.00742E7), List(22.55, 22.31, 2.40721E7), List(22.63, 22.35, 3.425...

data3.size
//res39: Int = 3000

val finalData = data3.map{
     | case (List(param1, param2, param3)) => List(1-param2/param1 , param3)}
//finalData: Array[List[Double]] = Array(List(0.011194029850746356, 2.50437E7), List(0.009367681498829161, 4.00198E7), List(0.015413358243811404, 4.86123E7), List(0.011617100371747235, 3.7566E7), List(0.008837209302325677, 4.10914E7), List(0.011167985109353129, 4.06955E7), List(0.011644154634373516, 4.42997E7), List(0.01949860724233976, 5.64347E7), List(0.023941068139963106, 6.59554E7), List(0.019824804057169243, 8.54868E7), List(0.03125000000000011, 2.430643E8), List(0.02041666666666664, 7.33284E7), List(0.01719798657718119, 3.80202E7), List(0.007633587786259555, 2.26949E7), List(0.01828231292517002, 3.08841E7), List(0.020797962648556823, 3.55029E7), List(0.014976465554129237, 4.81426E7), List(0.035698924731182746, 4.63271E7), List(0.009717314487632467, 2.86067E7), List(0.012345679012345...
//the final dataset is reduced to two variables: relative volatility and volume.

src.close()
//the file needs to be closed to avoid leaking of the file handle.


Step 3: Preprocessing data

Normalization is not always needed. 
But, if you believe the model can favorise a particular feature, then normalizing data imposes a single range of values for all the features. 
Normalization techniques include linear normalization and Z-score.

Linear normalization using min and max values, so that all the observations share the same scaling factor. Let's see how I can adapt Patrick's code to my modest data scientist programming skills. 

val volatility = finalData.map { 
     | case List(param1, param2) => param1}
//volatility: Array[Double] = Array(0.011194029850746356, 0.009367681498829161, 0.015413358243811404, 0.011617100371747235, 0.008837209302325677, 0.011167985109353129, 0.011644154634373516, 0.01949860724233976, 0.023941068139963106, 0.019824804057169243, 0.03125000000000011, 0.02041666666666664, 0.01719798657718119, 0.007633587786259555, 0.01828231292517002, 0.020797962648556823, 0.014976465554129237, 0.035698924731182746, 0.009717314487632467, 0.012345679012345734, 0.011889035667106973, 0.012205754141238034, 0.016136066288704787, 0.010643015521064392, 0.012372956252761691, 0.01471243869817207, 0.019434628975265045, 0.025217391304347747, 0.012981393336218172, 0.016421780466724267, 0.020104895104894993, 0.021030042918455005, 0.010674637062339842, 0.012414383561643816, 0.01753635585970914, ...

val volume = finalData.map {
     | case List(param1, param2) => param2}
//volume: Array[Double] = Array(2.50437E7, 4.00198E7, 4.86123E7, 3.7566E7, 4.10914E7, 4.06955E7, 4.42997E7, 5.64347E7, 6.59554E7, 8.54868E7, 2.430643E8, 7.33284E7, 3.80202E7, 2.26949E7, 3.08841E7, 3.55029E7, 4.81426E7, 4.63271E7, 2.86067E7, 3.33287E7, 3.44974E7, 3.04656E7, 3.00742E7, 2.40721E7, 3.42534E7, 5.01303E7, 4.90214E7, 6.4553E7, 3.45946E7, 4.44866E7, 6.93663E7, 4.0483E7, 2.62012E7, 2.91031E7, 2.77738E7, 3.88175E7, 4.54383E7, 3.18657E7, 2.95759E7, 3.36078E7, 3.81611E7, 4.00847E7, 3.01376E7, 4.03357E7, 8.24555E7, 5.45779E7, 2.65638E7, 2.97624E7, 2.69103E7, 3.74204E7, 4.00909E7, 2.95413E7, 1.99995E7, 3.21199E7, 1.85099E7, 2.234E7, 2.51532E7, 2.93024E7, 2.27447E7, 2.86158E7, 2.3926E7, 2.74439E7, 3.1875E7, 3.52484E7, 2.66055E7, 2.80359E7, 3.97687E7, 3.57085E7, 4.12686E7, 2.4327E7, 3.33...
 
val minMaxVolatility = (volatility.min, volatility.max) 
//minMaxVolatility: (Double, Double) = (4.115226337448874E-4,0.1340153452685422)

val minMaxVolume = (volume.min, volume.max)
//minMaxVolume: (Double, Double) = (7099200.0,5.600402E8)


val normalizedVolatility = volatility.map(
     | x => (x-minMaxVolatility._1)/(minMaxVolatility._2 - minMaxVolatility._1))
//normalizedVolatility: Array[Double] = Array(0.08070508017180902, 0.06703519920658041, 0.11228597591158493, 0.08387168508368592, 0.06306471253904308, 0.0805101400804284, 0.0840741812555225, 0.14286331208328476, 0.1761143135143344, 0.14530483515048873, 0.23082032203937328, 0.14973481774997793, 0.12564359022362467, 0.05405582722177045, 0.13375957318432788, 0.1525887479322935, 0.10901591461344078, 0.2641197040738504, 0.06965213771857956, 0.08932496199021613, 0.08590707067368558, 0.08827765010685647, 0.11769531249074021, 0.07658083942169104, 0.0895291271097316, 0.10703972223548107, 0.14238444654027108, 0.18566735727621417, 0.09408316659346425, 0.11983383047910999, 0.14740126504449985, 0.15432582599878392, 0.07681752083283515, 0.08983920288499864, 0.12817622196914646, 0.1525832420107761, 0.15...

val normalizedVolume = volume.map(
     | x => (x-minMaxVolume._1)/(minMaxVolume._2 - minMaxVolume._1))
//normalizedVolume: Array[Double] = Array(0.032452829506222185, 0.059537274320406695, 0.07507690693943839, 0.055099549499856224, 0.06147527493891753, 0.06075928534870809, 0.0672775214715494, 0.08922380507142715, 0.10644209780066952, 0.14176485375474057, 0.42674552981240316, 0.11977625099242054, 0.05592097529392828, 0.02820499836329735, 0.043015258409125026, 0.05136841001119469, 0.07422744922152635, 0.07094409710981822, 0.03889655496698563, 0.04743634492649306, 0.04954995198402723, 0.04225839646544568, 0.04155054517570591, 0.03069568000925958, 0.049108675247449544, 0.07782222696454053, 0.07581676887769219, 0.10390584167207713, 0.04972573927417211, 0.06761553221772305, 0.1126107487055581, 0.060374976715418104, 0.034546181238142945, 0.039794299934351046, 0.037390245975610414, 0.0573629012860...


Step 4: Discovering patterns

Plotting variables:
I'll use breeze-viz for this task with code from Pascal Bugnion's book.

import breeze.plot._
//import breeze.plot._

val fig = Figure()
//fig: breeze.plot.Figure = breeze.plot.Figure@49c2fa79

val plt = fig.subplot(0)
//plt: breeze.plot.Plot = breeze.plot.Plot@23373a7e

plt += plot(normalizedVolatility, normalizedVolume, '+')
//res4: breeze.plot.Plot = breeze.plot.Plot@23373a7e

Now, back to our logistic regression model. 

Step 5: Implementing the classifier

I'll use Spark-ML for implementing the logistic regression model.   

import org.apache.spark.ml.classification.LogisticRegression
//import org.apache.spark.ml.classification.LogisticRegression

val lr = new LogisticRegression().
     | setMaxIter(10).
     | setRegParam(0.3).
     | setElasticNetParam(0.8)
//lr: org.apache.spark.ml.classification.LogisticRegression = logreg_6495c4928687

Now, I need a Dataset[_] to fit in the method: val lrModel = lr.fit()

import org.apache.spark.SparkConf
//import org.apache.spark.SparkConf

val conf = new SparkConf().setMaster("local").setAppName("my app")
//conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@720498b5

val spark = SparkSession.builder().config(conf=conf).getOrCreate()
//...
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3d7b9536

import spark.implicits._
//import spark.implicits._

val ds = (volatility.zip(volume)).toSeq.toDS
// error 
Ok I guess I have to give some definition of the hyperplane to create the two classes. Maybe I should use the decision boundary from the plot. I'll check Pascal's book for additional support. For the sake of having a reusable code I've created the object called ExampleLR, stored in ExampleLR.scala, which will be further developed during today's routine. Now, back to REPL to see how things work with the logistic regression endeavor. 

var myds = normalizedVolatility.zip(normalizedVolume)
//myds: Array[(Double, Double)] = Array((0.08070508017180902,0.032452829506222185), (0.06703519920658041,0.059537274320406695), (0.11228597591158493,0.07507690693943839), (0.08387168508368592,0.055099549499856224), (0.06306471253904308,0.06147527493891753), (0.0805101400804284,0.06075928534870809), (0.0840741812555225,0.0672775214715494), (0.14286331208328476,0.08922380507142715), (0.1761143135143344,0.10644209780066952), (0.14530483515048873,0.14176485375474057), (0.23082032203937328,0.42674552981240316), (0.14973481774997793,0.11977625099242054), (0.12564359022362467,0.05592097529392828), (0.05405582722177045,0.02820499836329735), (0.13375957318432788,0.043015258409125026), (0.1525887479322935,0.05136841001119469), (0.10901591461344078,0.07422744922152635), (0.2641197040738504,0.0709440...

myds.map{ case x => if (x._1 <0.54 & x._2<0.23) Seq( x._1, x._2,0)} 
//res30: Array[Any] = Array(List(0.08070508017180902, 0.032452829506222185, 0.0), List(0.06703519920658041, 0.059537274320406695, 0.0), List(0.11228597591158493, 0.07507690693943839, 0.0), List(0.08387168508368592, 0.055099549499856224, 0.0), List(0.06306471253904308, 0.06147527493891753, 0.0), List(0.0805101400804284, 0.06075928534870809, 0.0), List(0.0840741812555225, 0.0672775214715494, 0.0), List(0.14286331208328476, 0.08922380507142715, 0.0), List(0.1761143135143344, 0.10644209780066952, 0.0), List(0.14530483515048873, 0.14176485375474057, 0.0), (), List(0.14973481774997793, 0.11977625099242054, 0.0), List(0.12564359022362467, 0.05592097529392828, 0.0), List(0.05405582722177045, 0.02820499836329735, 0.0), List(0.13375957318432788, 0.043015258409125026, 0.0), List(0.1525887479322935, ...

myds.map{ case x => if (x._1 <0.54 & x._2<0.23) Seq( x._1, x._2,0) else Seq(x._1, x._2, 1)}
//res31: Array[Seq[Double]] = Array(List(0.08070508017180902, 0.032452829506222185, 0.0), List(0.06703519920658041, 0.059537274320406695, 0.0), List(0.11228597591158493, 0.07507690693943839, 0.0), List(0.08387168508368592, 0.055099549499856224, 0.0), List(0.06306471253904308, 0.06147527493891753, 0.0), List(0.0805101400804284, 0.06075928534870809, 0.0), List(0.0840741812555225, 0.0672775214715494, 0.0), List(0.14286331208328476, 0.08922380507142715, 0.0), List(0.1761143135143344, 0.10644209780066952, 0.0), List(0.14530483515048873, 0.14176485375474057, 0.0), List(0.23082032203937328, 0.42674552981240316, 1.0), List(0.14973481774997793, 0.11977625099242054, 0.0), List(0.12564359022362467, 0.05592097529392828, 0.0), List(0.05405582722177045, 0.02820499836329735, 0.0), List(0.133759573184327...


val finalDS =myds.map{ case x => if (x._1 <0.54 & x._2<0.23) Seq(0, x._1, x._2) else Seq(1, x._1, x._2)}
//finalDS: Array[Seq[Double]] = Array(List(0.0, 0.08070508017180902, 0.032452829506222185), List(0.0, 0.06703519920658041, 0.059537274320406695), List(0.0, 0.11228597591158493, 0.07507690693943839), List(0.0, 0.08387168508368592, 0.055099549499856224), List(0.0, 0.06306471253904308, 0.06147527493891753), List(0.0, 0.0805101400804284, 0.06075928534870809), List(0.0, 0.0840741812555225, 0.0672775214715494), List(0.0, 0.14286331208328476, 0.08922380507142715), List(0.0, 0.1761143135143344, 0.10644209780066952), List(0.0, 0.14530483515048873, 0.14176485375474057), List(1.0, 0.23082032203937328, 0.42674552981240316), List(0.0, 0.14973481774997793, 0.11977625099242054), List(0.0, 0.12564359022362467, 0.05592097529392828), List(0.0, 0.05405582722177045, 0.02820499836329735), List(0.0, 0.13375957...

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)

val rddDS = sc.parallelize(finalDS)
sc.stop()

import org.apache.spark.sql._

val spark = SparkSession.builder().config(conf = conf).getOrCreate()

import spark.implicits._

val dsLR = spark.createDataset(finalDS)
//res3: org.apache.spark.sql.Dataset[Seq[Double]] = [value: array<double>]

So, I've finally come to a Dataset type to use in the lr.fit().

val lr = new LogisticRegression()
//res0: org.apache.spark.ml.classification.LogisticRegression = logreg_5eba13ff09dc

val lrModel = lr.fit(dsLR)
//error

Ok, I guess I need to label the Dataset with a LabeledPoint object, because my labelling is not yet recognized by Spark:

import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.{Vector, Vectors}

val parsedDS = rddDS.map{ list =>
     | LabeledPoint(list(0), Vectors.dense(list(1), list(2)))}
//parsedDS: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:19
 
parsedDS.first
//...
res9: org.apache.spark.mllib.regression.LabeledPoint = (0.0,[0.08070508017180902,0.032452829506222185])

This data structure is needed by the Spark's logistic regression algorithm.


val lrModel = new LogisticRegressionWithLBFGS().setNumClasses(2).run(parsedDS)
//...
lrModel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = 0.5

Now, I should not only train the model but also test and validate it. So, I will split the original dataset into two parts: trainDS (60% of the obs.) and testDS (40% of the obs.). Then I will rerun the LR algorithm, this time on trainDS. Afterwards I will classify the obs. found in the testDS and see the error. 

Step 6: Classifying new observations and assessing the quality of the model

For the moment, let's just sum up how the MLlib works in general. MLlib lets you invoke various algorithms on RDDs (the parsedDS for example is an RDD[LabeledPoint]). The features must be in numerical form (so, if you have an RDD of strings you must convert it to numerical form - process called feature extraction). Further, you transform the numerical features to vectors. Then, if you have a classification problem for example, you call a classification algorithm (e.g. logistic regression) on the RDD of vectors. This will return a model object that can be used to classify new points. At the end you evaluate the model on a test dataset. 

The above workflow steps represent the "learning pipeline". You can use the pipeline API for building such pipelines. 

The mllib.feature package contains algorithms to construct feature vectors from text, and ways to normalize and scale features. 

Now, I would like to further concentrate on neural learning, which is the end goal of my study process this sommer (or autumn?). So, back to "Grokking Deep Learning". 

_____________________________________________________________

ch. Gradient Descent ("Grokking Deep Learning" - Andrew Trask)

Different ways of measuring error prioritize error differently. 
Calculate both the direction and amount of error => Gradient Descent
Learning is calculating weight_delta and putting it on the weight. 
Alpha allows us to control how fast the network learns given weight_delta and it is the simplest way to prevent overcorrections of weights.

Given a certain input and goal_prediction, there is an exact relatioship defined between the error and weight. According to it, moving the weight moves the error. 
For squared errors the relationship is: error = ((input * weight) - goal_prediction) **2. This relationship between error and weight can be graphed as a parabola. Adjusting the weight to reduce the error means that we create a function which conforms to the patterns in the data.   
 
The way the error is controled by weight is described by the derivative of their relationship (we can of course have a positive and a negative derivative).  
  
This method of learning (using the derivative to find the lowest weight) is called Gradient Descent.

Now, I will try to implement the GD in Scala. The alternative is to use the already implemented one of the Spark MLlib.


@annotation.tailrec
def updatingWeights (weight: Double, goalPred:Double,
                     input: Double, alpha :Double) : Double = {

  var prediction = input * weight
  var error = prediction - goalPred
  var derivative = input* (prediction - goalPred)


    if (derivative < 0.01) weight
    else updatingWeights(weight, goalPred, input, alpha)

}
 
Ok, it seams my function doesn't work for small values of the goal_derivative. So, I've set the threshold at 0.5 for the moment. So, the "improved" version is:

@annotation.tailrec
def updatingWeights (weight: Double, goalPred:Double,
                     input: Double, alpha :Double) : Unit = {

  var prediction = input * weight
  var derivative = input* (prediction - goalPred)
  var error = prediction - goalPred

    if (derivative < 0.5) println(prediction , error)
    else updatingWeights(weight, goalPred, input, alpha)

}

updatingWeights(0.5, 0.8, 2, 0.1)
//(1.0,0.19999999999999996)   


A better-working version is:

@annotation.tailrec
def updatingWeights (weight: Double, goalPred:Double,
                     input: Double, alpha :Double) : Unit = {

  var prediction = input * weight
  var error = prediction - goalPred
  var derivative = input* (prediction - goalPred)

    if (error < 0.01) println(prediction , error)
    else updatingWeights(weight - alpha * derivative, goalPred, input, alpha)

}

updatingWeights(0.5, 0.8, 2, 0.1)
//(0.8093312,0.009331199999999984)
// Yes!! It looks good!

or even better:

@annotation.tailrec
def updatingWeights (weight: Double, goalPred:Double,
                     input: Double, alpha :Double) : Unit = {

  var prediction = input * weight
  var error = prediction - goalPred
  var derivative = input* (prediction - goalPred)

    if (error < 0.001) println(prediction , error, weight)
    else updatingWeights(weight - alpha * derivative, goalPred, input, alpha)

}

updatingWeights(0.5, 0.8, 2, 0.1)  
//(0.8007255941120001,7.255941120000164E-4,0.40036279705600003)
// (prediction, error, weight)


  
If you want to see the whole list of iterations (it takes 13 iterations):

@annotation.tailrec
def updatingWeights (weight: Double, goalPred:Double,
                     input: Double, alpha :Double) : Unit = {

  var prediction = input * weight
  var error = prediction - goalPred
  var derivative = input* (prediction - goalPred)
  println(prediction , error, weight)
    if (error < 0.001) println(prediction , error, weight)
    else  updatingWeights(weight - alpha * derivative, goalPred, input, alpha)

}

updatingWeights(0.5, 0.8, 2, 0.1)

//(1.0,0.19999999999999996,0.5)
(0.92,0.12,0.46)
(0.872,0.07199999999999995,0.436)
(0.8432000000000001,0.043200000000000016,0.42160000000000003)
(0.8259200000000001,0.025920000000000054,0.41296000000000005)
(0.815552,0.01555200000000001,0.407776)
(0.8093312,0.009331199999999984,0.4046656)
(0.80559872,0.005598719999999946,0.40279936)
(0.803359232,0.0033592319999999898,0.401679616)
(0.8020155392,0.0020155391999999717,0.4010077696)
(0.8012093235200001,0.0012093235200000274,0.40060466176000004)
(0.8007255941120001,7.255941120000164E-4,0.40036279705600003)
(0.8007255941120001,7.255941120000164E-4,0.40036279705600003)


_____________________________________________________________

ch. Learning multiple weights at a time - Generalizing Gradient Descent 
("Grokking Deep Learning" - Andrew Trask)

I've summed up the information in an object called GradientDescentMultiInput found in the GradientDescentMultiInput.scala file. 

object GradientDescentMultiInput {

import breeze.linalg._
import scala.math._

val myW = new DenseVector(Array(0.1, 0.2, -0.1))
val myI = new DenseVector(Array(8.5, 0.65, 1.2))
val goalPredict = new DenseVector(Array(1,1,0,1))
@annotation.tailrec
def updatingWeights (myWeights: DenseVector[Double], goalPred:Double,
                     myInputs: DenseVector[Double], alpha :Double) : Unit = {

  def predict (myInputs: DenseVector[Double], myWeights: DenseVector[Double]) : Double= {
val dotProduct = myInputs dot myWeights
dotProduct
}
  var error = predict(myInputs, myWeights) - goalPredict(0)
  var derivative = myInputs* (predict(myInputs, myWeights) - goalPredict(0))

    if (abs(error) < 0.001) println(predict(myInputs, myWeights) , error, myWeights)
    else updatingWeights(myWeights - alpha * derivative, goalPred, myInputs, alpha)
}

}

If I use the updatingWeights() function I get the following output in REPL:
 
updatingWeights(myW, 1.0, myI, 0.01)
//(0.9993712348198351,-6.287651801648586E-4,DenseVector(0.11598455720652519, 0.20122234849226373, -0.09774335662966704))
 
_______________________________________________

Creating a package to better organize the code:

scala> import com.incds.scalaML._
import com.incds.scalaML._

scala> kNN.
classes   classifykNN   createData   dataSet   group   lables   result

It can be directly imported, and easily accessed.
I've used the directory structure found in "Scala for Machine Learning" - Patrick R. Nicolas, 2015. 

I've developed a readFileClassif() function to read txt and csv files for the kNN classifier. A first version is:

def readFileClassif (filename: String, delim: String) :  (Vector[Array[Double]], Vector[Double],  Range.Partial[Double,scala.collection.immutable.NumericRange[Double]]
)= {

val src = scala.io.Source.fromFile(filename)
val data = src.getLines.map(_.split(delim)).toArray.drop(1)
src.close()
val len = data(0).length
val data2 = data.map( for (i <- _) yield(i))

val data3= data2.map( for (i <- _) yield (i.toDouble))

val dataMatrix= data3.map{ case x => x.take(len-1)}.toVector

val dataLabels = data3.map{ case x => x.last}.toVector

val classes = (dataLabels.min to dataLabels.max)

(dataMatrix, dataLabels, classes)
}



Now, I would like to connect the two pieces, so they can be used together. I will take a look at types and harmonize them.
 
_________________________________________________

Rewriting the distance() function to generalize the computation of the Euclidean distance between any two vectors of the same length.  

scala> P1
res27: scala.collection.immutable.Vector[Array[Double]] = Vector(Array(77450.0, 14.856391, 1.129823))

scala> P2
res28: scala.collection.immutable.Vector[Array[Double]] = Vector(Array(46426.0, 7.292202, 0.548607))

scala> val tuple=(P1, P2)
tuple: (scala.collection.immutable.Vector[Array[Double]], scala.collection.immutable.Vector[Array[Double]]) = (Vector(Array(77450.0, 14.856391, 1.129823)),Vector(Array(46426.0, 7.292202, 0.548607)))

scala> tuple._1(0)
res29: Array[Double] = Array(77450.0, 14.856391, 1.129823)

scala> tuple._1
res30: scala.collection.immutable.Vector[Array[Double]] = Vector(Array(77450.0, 14.856391, 1.129823))

scala> tuple._1(0)(0)
res31: Double = 77450.0

scala> tuple._2(0)(0)
res32: Double = 46426.0


scala> val featuresP1 = for (i <- P1(0)) yield i 
featuresP1: Array[Double] = Array(77450.0, 14.856391, 1.129823)

scala> featuresP1(0)
res52: Double = 77450.0

scala> val featuresP2 = for (i <- P2(0)) yield i
featuresP2: Array[Double] = Array(46426.0, 7.292202, 0.548607)

scala> import breeze.linalg._
import breeze.linalg._

scala> featuresP1 - featuresP2
res53: Array[Double] = Array(31024.0, 7.564189000000001, 0.5812160000000001)

scala> val pointDiff= featuresP1 - featuresP2
pointDiff: Array[Double] = Array(31024.0, 7.564189000000001, 0.5812160000000001)

scala> for (i <- pointDiff) yield pow(i, 2)
res54: Array[Double] = Array(9.62488576E8, 57.21695522772101, 0.33781203865600007)

scala> val pointDiffPow = for (i <- pointDiff) yield pow(i, 2)
pointDiffPow: Array[Double] = Array(9.62488576E8, 57.21695522772101, 0.33781203865600007)

scala> val d = sqrt(sum(pointDiffPow))
d: Double = 31024.000927584555

Now, I will insert the changes into the kNN.scala file. 

________________________________________________________________









