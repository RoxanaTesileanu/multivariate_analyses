References:
"Scala for Machine Learning" - Patrick R. Nicolas
"Scala for Data Science" - Pascal Bugnion
ch.1 Getting started

ML problems are categorized as classifications, prediction, optimization and regression.
Classification - example: identify a disease from a set of symptoms
Prediction - once the model is trained and validated, it can be used to predict some outcomes (a doctor collects symptoms and anticipates the health state).
Optimization - ML techniques increase the chances that the optimization method converges towards a solution (intelligent search) (example: fighting the spread of a new virus requires optimizing a process that may evolve over time as more symptoms and cases are uncovered).
Regression: is a classification technique particularly suitable for a continuous model.

Why Scala? 
- abstraction: monads (derived from the category and group theory to create high-level abstractions); covariant, contravariant functors and bifunctors (related to manifold and vector bundles used in non-linear model building).    

- higher-kind projection: classification problems rely on the estimation of the similarity between vectors of observations. One technique consists in comparing two vectors by computing the normalized inner product. A co-vector alpha is defined as a linear map of a vector to the inner product. 

- covariant functors for vectors: F[U => V] := F[U] => F[V]
- contravariant functors for co-vector: F[U => V] := F[V] => F[U]
- monads: extend the concept of a functor to allow the composition known as the monadic composition of morphisms on a single type.

- scalability: parallelization and chaining of data processing functions using higher-order methods. So one can build scalable, distributed and concurrent applications (using Akka or Apache Spark). 

Model categorization
A model can be predictive, descriptive or adaptive.

- predictive models: discover patterns in the data and extract fundamental trends and relationships between factors (or features). They are used to predict and classify future events or observations. Predictive models are created through supervised learning using a training set.

- descriptive models: attempt to find unusual patterns or affinities in data by grouping observations into clusters with similar properties. They are generated through unsupervised learning.

- adaptive models: created through reinforcement learning (one or more decision-making agents that recommend and possibly execute actions in the attempt of solving a problem, optimizing an objective function, or resolving constraints.

Taxonomy of machine learning algorithms

data mining -> identifying patterns
machine learning -> indentifying and validating models to optimize a performance criterion using data.

A) Unsupervised learning
The goal of unsupervised learning is to discover patterns of regularities and irregularities in a set of observations. There are two categories: discovery of data clusters and discovery of latent factors.
Unsupervised learning does not require labeled data and therefore is easy to implement and execute because no expertise is needed to validate an output. 

A1) Clustering
 
The purpose of data clustering is to partition a collection of data into a number of clusters or data segments. Practically, a clustering algorithm is used to organize observations into clusters, by minimizing the distance between obs. within a cluster and maximizing the distance between obs. across clusters. 

A2) Dimension reduction

Dimension reduction aims at finding the smallest but most relevant set of features needed to build a reliable model. 

B) Supervised learning

There are two categories of supervised learning algorithms:
- generative models
- discriminative models

B1) Generative models
Generative models attempt to fit a joint probability, p(X,Y), of two events (or random variables) X and Y, representing two sets of observed and hidden x and y variables. 
Generative models are commonly introduced through Bayes' rule.

B2) Disciminative models

Contrary to generative models, discriminative models compute the conditional probability p(Y|X) directly, using the same algorithm for training and classification. 


C) Reinforcement learning

Reinforcement learning algorithms output a set of recommended actions for the adaptive system to execute. In its simplest form, this type of algorithms estimate the best course of action. The challenge is the recommended action or policy may depend on partially observable states.

___________________________________

For mathematics, linear algebra and optimization there are some well-established libraries to use, e.g. Apache Commons Math. 

Add the dependency to your build.sbt file:
// https://mvnrepository.com/artifact/org.apache.commons/commons-math3
libraryDependencies += "org.apache.commons" % "commons-math3" % "3.6"

In Commons Math you can find:
- functions, differentiation, and integral and ordinary differential equations
- statistics distributions
- linear and non-linear optimization
- dense and sparse vectors and matrices
- curve fitting, correlation and regression


For plotting you can use the JFreeChart library. Add the dependency to your build.sbt file:
// https://mvnrepository.com/artifact/org.jfree/jfreechart
libraryDependencies += "org.jfree" % "jfreechart" % "1.0.18"

______________________________ 
  
An overview of computational workflows to perform a runtime processing of a dataset:

1. Loading the dataset from files, databases, or any streaming devices
2. Splitting the data set for parallel data processing
3. Preprocessing data 
4. Applying the model to classify new data
5. Assessing the quality of the model

A similar sequence of tasks is used to extract a model from a training dataset:

1. Loading the dataset from files, databases, or any streaming devices
2. Splitting the data set for parallel data processing
3. Preprocessing data 
4. Selecting the training, testing and validation set from the input data
5. Extracting key features (extracting the model: supervised or unsupervised)
6. Validating the model and tuning the model 
7. Storing the model in a file or database so that it can be applied to future obs.

The first phase consists of extracting the model through clustering or training of a supervised learning algorithm. The model is then validated against test data for which the source is the same as the training set but with different observations. Once the model is created and validated, it can be used to classify real-time data or predict future behaviour. Real-world workflows are more complex and require dynamic configuration to allow experimentation of different models. Several alternative classifiers and different filtering algorithms are applied against input data, depending on the latent noise in the raw data.

Writing a simple workflow:
For the first example a simplified version of the binomial logistic regression as the chosen classifier is used as the stock-price-volume action is treated as a continuous or pseudo-continuous process.

Step 1: Scoping the problem

The dataset is in CSV format. The aim is to model the stock price using its daily volume and volatility. We will consider two variables: price and volume.

Step 2: Loading data

val src = scala.io.Source.fromFile("CSCO.csv")
//res0: scala.io.BufferedSource = non-empty iterator
 
val data = src.getLines.map(_.split(",")).toArray.drop(1)
//data: Array[Array[String]] = Array(Array(11/29/2013, 21.39, 21.44, 21.2, 21.25, 25043700, 21.25), Array(11/27/2013, 21.29, 21.35, 21.15, 21.27, 40019800, 21.27), Array(11/26/2013, 21.3, 21.41, 21.08, 21.21, 48612300, 21.21), Array(11/25/2013, 21.45, 21.52, 21.27, 21.27, 37566000, 21.27), Array(11/22/2013, 21.41, 21.5, 21.31, 21.46, 41091400, 21.46), Array(11/21/2013, 21.36, 21.49, 21.25, 21.47, 40695500, 21.47), Array(11/20/2013, 21.47, 21.47, 21.22, 21.23, 44299700, 21.23), Array(11/19/2013, 21.31, 21.54, 21.12, 21.42, 56434700, 21.42), Array(11/18/2013, 21.6, 21.72, 21.2, 21.29, 65955400, 21.29), Array(11/15/2013, 21.46, 21.69, 21.26, 21.54, 85486800, 21.54), Array(11/14/2013, 20.94, 21.44, 20.77, 21.37, 243064300, 21.37), Array(11/13/2013, 23.6, 24, 23.51, 24, 73328400, 24), Array(11...

data(0)
//res8: Array[String] = Array(11/29/2013, 21.39, 21.44, 21.2, 21.25, 25043700, 21.25)
// date, open, high, low, close, volume, adj_close

val data2 = data.map(x => List(x(2), x(3), x(5)))
//data2: Array[List[String]] = Array(List(21.44, 21.2, 25043700), List(21.35, 21.15, 40019800), List(21.41, 21.08, 48612300), List(21.52, 21.27, 37566000), List(21.5, 21.31, 41091400), List(21.49, 21.25, 40695500), List(21.47, 21.22, 44299700), List(21.54, 21.12, 56434700), List(21.72, 21.2, 65955400), List(21.69, 21.26, 85486800), List(21.44, 20.77, 243064300), List(24, 23.51, 73328400), List(23.84, 23.43, 38020200), List(23.58, 23.4, 22694900), List(23.52, 23.09, 30884100), List(23.56, 23.07, 35502900), List(23.37, 23.02, 48142600), List(23.25, 22.42, 46327100), List(22.64, 22.42, 28606700), List(22.68, 22.4, 33328700), List(22.71, 22.44, 34497400), List(22.94, 22.66, 30465600), List(22.93, 22.56, 30074200), List(22.55, 22.31, 24072100), List(22.63, 22.35, 34253400), List(22.43, 22.1, 5...
// get only the needed variables

val data3=data2.map{ case(List(param1, param2, param3))  => List(param1.toDouble, param2.toDouble, param3.toDouble)}
//data3: Array[List[Double]] = Array(List(21.44, 21.2, 2.50437E7), List(21.35, 21.15, 4.00198E7), List(21.41, 21.08, 4.86123E7), List(21.52, 21.27, 3.7566E7), List(21.5, 21.31, 4.10914E7), List(21.49, 21.25, 4.06955E7), List(21.47, 21.22, 4.42997E7), List(21.54, 21.12, 5.64347E7), List(21.72, 21.2, 6.59554E7), List(21.69, 21.26, 8.54868E7), List(21.44, 20.77, 2.430643E8), List(24.0, 23.51, 7.33284E7), List(23.84, 23.43, 3.80202E7), List(23.58, 23.4, 2.26949E7), List(23.52, 23.09, 3.08841E7), List(23.56, 23.07, 3.55029E7), List(23.37, 23.02, 4.81426E7), List(23.25, 22.42, 4.63271E7), List(22.64, 22.42, 2.86067E7), List(22.68, 22.4, 3.33287E7), List(22.71, 22.44, 3.44974E7), List(22.94, 22.66, 3.04656E7), List(22.93, 22.56, 3.00742E7), List(22.55, 22.31, 2.40721E7), List(22.63, 22.35, 3.425...

data3.size
//res39: Int = 3000

val finalData = data3.map{
     | case (List(param1, param2, param3)) => List(1-param2/param1 , param3)}
//finalData: Array[List[Double]] = Array(List(0.011194029850746356, 2.50437E7), List(0.009367681498829161, 4.00198E7), List(0.015413358243811404, 4.86123E7), List(0.011617100371747235, 3.7566E7), List(0.008837209302325677, 4.10914E7), List(0.011167985109353129, 4.06955E7), List(0.011644154634373516, 4.42997E7), List(0.01949860724233976, 5.64347E7), List(0.023941068139963106, 6.59554E7), List(0.019824804057169243, 8.54868E7), List(0.03125000000000011, 2.430643E8), List(0.02041666666666664, 7.33284E7), List(0.01719798657718119, 3.80202E7), List(0.007633587786259555, 2.26949E7), List(0.01828231292517002, 3.08841E7), List(0.020797962648556823, 3.55029E7), List(0.014976465554129237, 4.81426E7), List(0.035698924731182746, 4.63271E7), List(0.009717314487632467, 2.86067E7), List(0.012345679012345...
//the final dataset is reduced to two variables: relative volatility and volume.

src.close()
//the file needs to be closed to avoid leaking of the file handle.


Step 3: Preprocessing data

Normalization is not always needed. 
But, if you believe the model can favorise a particular feature, then normalizing data imposes a single range of values for all the features. 
Normalization techniques include linear normalization and Z-score.

Linear normalization using min and max values, so that all the observations share the same scaling factor. Let's see how I can adapt Patrick's code to my modest data scientist programming skills. 

val volatility = finalData.map { 
     | case List(param1, param2) => param1}
//volatility: Array[Double] = Array(0.011194029850746356, 0.009367681498829161, 0.015413358243811404, 0.011617100371747235, 0.008837209302325677, 0.011167985109353129, 0.011644154634373516, 0.01949860724233976, 0.023941068139963106, 0.019824804057169243, 0.03125000000000011, 0.02041666666666664, 0.01719798657718119, 0.007633587786259555, 0.01828231292517002, 0.020797962648556823, 0.014976465554129237, 0.035698924731182746, 0.009717314487632467, 0.012345679012345734, 0.011889035667106973, 0.012205754141238034, 0.016136066288704787, 0.010643015521064392, 0.012372956252761691, 0.01471243869817207, 0.019434628975265045, 0.025217391304347747, 0.012981393336218172, 0.016421780466724267, 0.020104895104894993, 0.021030042918455005, 0.010674637062339842, 0.012414383561643816, 0.01753635585970914, ...

val volume = finalData.map {
     | case List(param1, param2) => param2}
//volume: Array[Double] = Array(2.50437E7, 4.00198E7, 4.86123E7, 3.7566E7, 4.10914E7, 4.06955E7, 4.42997E7, 5.64347E7, 6.59554E7, 8.54868E7, 2.430643E8, 7.33284E7, 3.80202E7, 2.26949E7, 3.08841E7, 3.55029E7, 4.81426E7, 4.63271E7, 2.86067E7, 3.33287E7, 3.44974E7, 3.04656E7, 3.00742E7, 2.40721E7, 3.42534E7, 5.01303E7, 4.90214E7, 6.4553E7, 3.45946E7, 4.44866E7, 6.93663E7, 4.0483E7, 2.62012E7, 2.91031E7, 2.77738E7, 3.88175E7, 4.54383E7, 3.18657E7, 2.95759E7, 3.36078E7, 3.81611E7, 4.00847E7, 3.01376E7, 4.03357E7, 8.24555E7, 5.45779E7, 2.65638E7, 2.97624E7, 2.69103E7, 3.74204E7, 4.00909E7, 2.95413E7, 1.99995E7, 3.21199E7, 1.85099E7, 2.234E7, 2.51532E7, 2.93024E7, 2.27447E7, 2.86158E7, 2.3926E7, 2.74439E7, 3.1875E7, 3.52484E7, 2.66055E7, 2.80359E7, 3.97687E7, 3.57085E7, 4.12686E7, 2.4327E7, 3.33...
 
val minMaxVolatility = (volatility.min, volatility.max) 
//minMaxVolatility: (Double, Double) = (4.115226337448874E-4,0.1340153452685422)

val minMaxVolume = (volume.min, volume.max)
//minMaxVolume: (Double, Double) = (7099200.0,5.600402E8)


val normalizedVolatility = volatility.map(
     | x => (x-minMaxVolatility._1)/(minMaxVolatility._2 - minMaxVolatility._1))
//normalizedVolatility: Array[Double] = Array(0.08070508017180902, 0.06703519920658041, 0.11228597591158493, 0.08387168508368592, 0.06306471253904308, 0.0805101400804284, 0.0840741812555225, 0.14286331208328476, 0.1761143135143344, 0.14530483515048873, 0.23082032203937328, 0.14973481774997793, 0.12564359022362467, 0.05405582722177045, 0.13375957318432788, 0.1525887479322935, 0.10901591461344078, 0.2641197040738504, 0.06965213771857956, 0.08932496199021613, 0.08590707067368558, 0.08827765010685647, 0.11769531249074021, 0.07658083942169104, 0.0895291271097316, 0.10703972223548107, 0.14238444654027108, 0.18566735727621417, 0.09408316659346425, 0.11983383047910999, 0.14740126504449985, 0.15432582599878392, 0.07681752083283515, 0.08983920288499864, 0.12817622196914646, 0.1525832420107761, 0.15...

val normalizedVolume = volume.map(
     | x => (x-minMaxVolume._1)/(minMaxVolume._2 - minMaxVolume._1))
//normalizedVolume: Array[Double] = Array(0.032452829506222185, 0.059537274320406695, 0.07507690693943839, 0.055099549499856224, 0.06147527493891753, 0.06075928534870809, 0.0672775214715494, 0.08922380507142715, 0.10644209780066952, 0.14176485375474057, 0.42674552981240316, 0.11977625099242054, 0.05592097529392828, 0.02820499836329735, 0.043015258409125026, 0.05136841001119469, 0.07422744922152635, 0.07094409710981822, 0.03889655496698563, 0.04743634492649306, 0.04954995198402723, 0.04225839646544568, 0.04155054517570591, 0.03069568000925958, 0.049108675247449544, 0.07782222696454053, 0.07581676887769219, 0.10390584167207713, 0.04972573927417211, 0.06761553221772305, 0.1126107487055581, 0.060374976715418104, 0.034546181238142945, 0.039794299934351046, 0.037390245975610414, 0.0573629012860...


Step 4: Discovering patterns

Plotting variables:
I'll use breeze-viz for this task with code from Pascal Bugnion's book.

import breeze.plot._
//import breeze.plot._

val fig = Figure()
//fig: breeze.plot.Figure = breeze.plot.Figure@49c2fa79

val plt = fig.subplot(0)
//plt: breeze.plot.Plot = breeze.plot.Plot@23373a7e

plt += plot(normalizedVolatility, normalizedVolume, '+')
//res4: breeze.plot.Plot = breeze.plot.Plot@23373a7e

Now, back to our logistic regression model. 

Step 5: Implementing the classifier




