\documentclass {article}

\title {Bayesian Notes for building the geostatistical MANOVA-KNN pipeline}
\date {November 2017}

\usepackage {amsmath, amsfonts}
\usepackage {mathtools}
\usepackage {graphicx}
\usepackage {verbatim}
\usepackage {cite}
\usepackage {booktabs}
\usepackage {float}
\usepackage [titletoc, toc, title] {appendix}
\usepackage {hyperref}
\usepackage [T1]{fontenc}
\usepackage {halloweenmath}
\usepackage {tabto}
\usepackage {amssymb}
\usepackage {array}
\author {Roxana Tesileanu \\
\\
\href{mailto: roxana.te@web.de}{roxana.te@web.de} \\
INCDS, Romania}

\begin {document}
	\maketitle
	\pagenumbering{gobble}
\tableofcontents

\newpage
\pagenumbering {arabic}
\section {Introduction}

From Gelman et al. 2014
\\
\\
BAYESIAN INFERENCE is the process of fitting a probability model to a set of data and SUMMARIZING THE RESULT BY A PROBABILITY DISTRIBUTION ON:
\begin{enumerate}
\item THE PARAMETERS OF THE MODEL and on
\item THE UNOBSERVED QUANTITIES SUCH AS PREDICTIONS FOR NEW OBSERVATIONS.
\end{enumerate} 
=> make inferences from data using probability models for quantities we observe and for quantities we wish to learn. 
\textbf{THE ESSENTIAL CHARACTERISTIC OF BAYESIAN MODELS IS THEIR EXPLICIT USE OF PROBABILITY FOR QUATIFYING UNCERTAINTY IN INFERENCES BASED ON STATISTICAL DATA ANALYSIS}. This is the main idea of the MANOVA-KNN pipeline, to analyse errors and reduce them. I've already sketched a geometric approach. It needs to be better backed up by probability theory. Which is what I expect to find in these Bayesian texts. 
\\
\\ 
Steps of Bayesian Data Analysis:
\begin{enumerate}
\item setting up A FULL PROBABILITY MODEL - a JOINT PROBABILITY DISTRIBUTION FOR ALL OBSERVABLE AND UNOBSERVABLE QUANTITIES IN A PROBLEM. 
\item CONDITIONING ON OBSERVED DATA - calculating and interpreting the appropriate POSTERIOR DISTRIBUTION (which is the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data). 
\item EVALUATING THE FIT OF THE MODEL AND THE IMPLICATIONS OF THE RESULTING POSTERIOR DISTRIBUTION: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? \textbf{IN RESPONSE, ONE CAN ALTER OR EXPAND THE MODEL AND REPEAT THE THREE STEPS}.
\end{enumerate}
Advances in carrying the third step alleviate to some degree the need to assume correct model specification at the first attempt. In particular, the much-feared dependence of conclusions on "subjective" prior distributions can be examined and explored.
A primary motivation of Bayesian thinking is that it facilitates a COMMON-SENSE INTERPRETATION OF STATISTICAL CONCLUSIONS. 
Also, the Bayesian paradigm provides a conceptually simple method for \textbf{coping with multiple paramerers}.  

\subsection{Notation for statistical inference}

\paragraph{Parameters, data and predictions}
\begin{itemize}
\item let $\theta$ denote unobservable vector quantities or POPULATION PARAMETERS OF INTEREST. 
\item $y$ denotes the observed data 
\item $\tilde{y}$ denotes unknown, but potentially observable, quantities 
\end{itemize}
In general these symbols represent MULTIVARIATE QUANTITIES. 
When using matrix notation, we consider vectors as column vectors throughout. 
Data are gathered on each of a set of n objects or units (like in classical multivariante analyses).
 And we can write the data as a vector: $y=(y_1, ..., y_n$).
 If several variables are measured on each unit, then each $y_i$ is a vector. 
\textbf{The entire dataset is a matrix with n rows}. The y variables are called the \textbf{"outcomes" and are considered "random"} in the sense that, when making inferences, we wish to allow for the possibility that the observed values of the variables could have turned out otherwise, due to the sampling process and the natural variation of the population.
We commonly model data from an exchangeable distribution as independently and identically distributed (iid) given some unknown parameter vector $\theta$ with distribution $p(\theta)$. 
The explanatory variables (covariates) are denoted with $x$.   
We use $X$ to denote the entire set of explanatory variables for all $n$ units. 
If there are $k$ explanatory variables, then $X$ is a matrix with $n$ rows and $k$ columns (similar to conventional notation from classical multivariate statistics).
$X$ is treated as random.

\paragraph{Bayesian inference}
Bayesian STATISTICAL CONCLUSIONS about a parameter $\theta$, or unobserved data $\tilde{y}$, are made in terms of PROBABILITY STATEMENTS.
These probability statements are conditional on the observed value of $y$, and in our notation they are simply written as: $p(\theta | y)$ or $p(\tilde{y} | y)$.
This is also written as $p(\cdot | \cdot)$. For a marginal distribution we use $p(\cdot)$. 
\\
\\
We may use the notation $Pr(\cdot)$ for the probability of an event (for example in $Pr(\theta > 2) = \int_{\theta >2} p(\theta) \mathrm{d}\theta$).
Also the probability density functions are denoted like for example: $\theta \sim N(\mu, \sigma^2)$ meaning that $\theta$ has a Normal distribution with mean $\mu$ and variance $\sigma^2$.
Equiv. we can write $p(\theta| \mu, \sigma^2)= N(\theta | \mu, \sigma^2)$.

\subsection {Bayes' Rule}
In order to make probability statements about $\theta$ given $y$, we must begin with a model providing a JOINT PROBABILITY DISTRIBUTION FOR $\theta$ and $y$. 
The joint probability mass or density function can be written as a PRODUCT OF TWO DENSITIES that are often referred to as the PRIOR DISTRIBUTION $p(\theta)$ and the SAMPLING DISTRIBUTION (or DATA DISTRIBUTION) $p(y| \theta)$: 
\begin{equation*}
p(\theta, y) = p(\theta)p(y | \theta)
\end{equation*}
Simply CONDITIONING ON THE KNOWN DATA, using the Bayes' Rule yields the POSTERIOR DENSITY:
\begin {equation*}
p(\theta | y) = \frac{p(\theta, y)}{p(y)}= \frac{p(\theta)p(y | \theta)}{p(y)}
\end{equation*}  
where $p(y)= \int p(\theta) p(y| \theta) \mathrm{d} \theta$ (for $\theta$ continuous), and $1/p(y)$ represents the constant of proportionality ensuring that the posterior density integrates to 1  as a proper probability density must \cite{jackman_bayesian_2009}. The constant of proportionality can be formally written as \cite{jackman_bayesian_2009}: 
\begin {equation*}
[\int p(y|\theta) p(\theta) \mathrm{d} \theta]^{-1}
\end {equation*}

Leaving the $1/p(y)$ off, we get the UNNORMALIZED POSTERIOR DENSITY which is the right side of:
\begin{equation*}
p(\theta | y) \propto p(\theta)p(y|\theta)
\end{equation*}
We can thus state that the \textbf{POSTERIOR IS PROPORTIONAL TO THE PRIOR TIMES THE LIKELIHOOD} \cite{jackman_bayesian_2009}.

\paragraph {Prediction}
Make inferences about an unknown observable (predictive inferences).
Before the data are considered, the distribution of the unknown but observable $y$ is:
\begin{equation*}
p(y) = \int p(y, \theta) \mathrm{d} \theta = \int p(\theta) p(y| \theta) \mathrm{d} \theta
\end{equation*}
This is called the \textbf{marginal distribution of} $\mathbf{y}$ and it is also called the \textbf{prior predictive distribution}.
After the data $y$ have been observed, we can predict an unknown observable, $\tilde{y}$, from the same process.
The distribution of $\tilde{y}$ is called the \textbf{posterior predictive distribution}. It is CONDITIONAL ON THE OBSERVED $y$:
\begin{equation*}
p(\tilde{y} | y) = \int p(\tilde{y}, \theta | y) \mathrm{d} \theta\\
 = \int p(\tilde{y} | \theta, y) p(\theta | y) \mathrm{d} \theta \\
 = \int p(\tilde{y} | \theta) p(\theta | y) \mathrm{d} \theta
\end{equation*}

\paragraph{Likelihood}
Using Bayes' Rule WITH A CHOSEN PROBABILITY MODEL means that the data $y$ affect the posterior inference only through $p(y | \theta)$, which is called the \textbf{likelihood function}. 

\paragraph {Example for a single-parameter problem}
From Jackman 2009.
\\
\\
Suppose we have the success probability $\theta \in [0,1]$ underlying a binomial process. There might be more combinations of a prior, a likelihood and a posterior distribution.
If we use a uniform prior, $\theta \sim Unif(0,1)$, the information of the prior is "absorbed" into the constant of proportionality, i.e. is an uninformative prior, resulting in a posterior density over $\theta$ that is proportional to the likelihood.      
\textbf{The Uniform prior is used when we have no prior information about the value of $\theta$ and hence no way to a priori prefer one set of values for $\theta$ over any other}.
Also note that likelihood based analyses of data (so with a uniform prior) assume prior ignorance (which must be plausible). 
\textbf{Usually, Bayesian inference works with more or less informative priors for $\theta$}.
In these cases, the mean of the posterior distribution is a \textbf{precision-weighted average} of the prior and the likelihood, i.e. it reduces the variance observed in the prior and likelihood.  
This is one of the consequences of working with so-called \textbf{conjugate priors} in the exponential family. 

\paragraph{Conjugate priors}
Priors represent convenient ways of mathematically expressing prior beliefs over parameters.
 Many statistical models are in the exponential family (but not all), for which conjugate priors are convenient ways of mathematically and computationally quite simple.
\\
\\
\textit{Definition: Suppose a prior density $p(\theta)$ belongs to a class of parametric densities F.
Then the prior density is said to be conjugate with respect to a likelihood $p(y|\theta)$ if the posterior density $p(\theta | y)$ is also in F.}  
Where the definition of a "class of parametric densities" will be made clear along the way.
\\
\\
An example is the use of a Beta prior with Binomial data (in the beta-binomial model).
 For the likelihood function formed with binomial data, any Beta density over $\theta$ is a conjugate prior.
 The idea of the beta-binomial model is that, if prior beliefs about $\theta$ can be represented as a Beta density, then after those beliefs have been updated (via Bayes Rule) in light of binomial data, posterior beliefs about $\theta$ are also characterized by a Beta density.  
\\
\\
When conjugacy holds, Bayesian analysis  
  

\section* {Note}
This document is "under construction".
 It contains older notes of mine on Bayesian data analysis.
 Some were used in technical reports of mine (see \href{https://www.researchgate.net/publication/317549069_poisson_model}{https://www.researchgate.net/publication/317549069\underline{\space}poisson\underline{\space}model}) and also new sections aiming at creating the background necessary for the implementation of the MANOVA-KNN pipeline in geostatistics using the idea of \textbf{posterior predictive checks} (Introduction and Deduction in Bayesian Data Analysis, Andrew Gelman, 2011) \cite{gelman_introduction_2011}.
 For this purpose, I will have to work through books building up my skills, fortunately I was given a hint (and a copy) by a friend on "Bayesian Data Analysis for Social Sciences" by Simon Jackman (Wiley, 2009) \cite{jackman_bayesian_2009} and "Bayesian Data Analysis" by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari and Donald B. Rubin (CRC, 2014) \cite{gelman_bayesian_2014}.    
Please download the current version from  my GitHub profile under the multivariate\underline{\space}analyses project repository: \href{https://github.com/RoxanaTesileanu/multivariate_analyses/blob/master/literature_analysis/geospatial_scala/bayesian_notes_geosp.tex}{https://github.com/RoxanaTes\\ileanu/multivariate\underline{\space}analyses/blob/master/literature\underline{\space}analysis/geospatial\underline{\space}scala/b\\ayesian\underline{\space}notes\underline{\space}geosp.pdf}. 
\\
\\
The statistical plots in this document were generated in Scala using the JavaPlot package developed by Panayotis Katsaloulis \cite{panayotis_javaplot_2017}. You can find the scala source files used for generating them under the link: 
\href{https://github.com/RoxanaTesileanu/multivariate_analyses/tree/master/DeepLearning/src/main/scala/com/mai/scalaPlot}{https://github.com/RoxanaTesilea\\nu/multivariate\underline{\space}analyses/tree/master/DeepLearning/src/main/scala/com/mai/\\scalaPlot}.  
\\
\\
The present document was edited using Latex \cite{claudio_latex-tutorial.com_nodate} (\href{https://www.latex-project.org/}{https://www.latex-project.org/}). The source .tex file of the present document is also available in the multivariate\underline{\space}analyses repository on my GitHub profile. Special thanks to Gustavo Mezzetti for the Latex halloweenmath package:
\href{http://mirrors.concertpass.com/tex-archive/macros/latex/contrib/halloweenmath/halloweenmath-man.pdf}{http://mirrors.concertpass.com/tex-archive/macro\\s/latex/contrib/halloweenmath/halloweenmath-man.pdf}!  
%\section* {References}
\bibliography {MyLibrary}
\bibliographystyle {IEEEtran}




\end {document}
