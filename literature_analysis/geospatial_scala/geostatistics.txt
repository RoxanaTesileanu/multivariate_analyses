Working through "Geostatistics for Environmental Scientists" (R. Webster, M.A. Oliver) 2007


Author: Roxana Tesileanu, roxana.te@web.de
Affiliation: INCDS, BV
Date: October 2017

Chapter 1: INTRODUCTION

- the environment is continuous, but in general we can afford to measure properties at only a finite number of places
-> the best we can do is to estimate, or predict, in a spatial sense.
-> this is the reason we use geostatistics; it allows us to deal with properties that vary in ways that are far from systematic and at all spatial scales. 

GEOSTATISTICS CAN NEVER PROVIDE COMPLETE INFORMATION, BUT, GIVEN THE DATA, IT CAN ENABLE YOU TO ESTIMATE THE PROBABILITIES THAT THE TRUE DATA ARE WITHIN SOME SPECIFIED BOUNDS. 
__________________________________________

Generalizing:  
The environment extends more or less continuously in two dimensions. It varies from place to place beeing marked by complex interrelated processes (panarchy of processes at fast to slow paces and from small to large scales). 
Obvious properties can be analysed from aerial photos or satellite imagery, others like soil properties cannot. For the later ones we rely on measurements and analysis of samples.

BY DESCRIBING THE VARIATION AT DIFFERENT SPATIAL RESOLUTIONS WE CAN OFTEN GAIN INSIGHT INTO THE PROCESSES AND FACTORS THAT CAUSE OR CONTROL IT, AND SO PREDICT IN A SPATIAL SENSE AND MANAGE RESOURCES.

An additional issue in geospatial analysis is that AT SOME SCALE THE VALUES OF PROPERTIES ARE AUTOCORRELATED.
Places close to one another tend to have similar values, whereas ones that are farther apart differ more on average. We have to QUANTIFY THE SPATIAL AUTOCORRELATION AT THE SCALE OF INTEREST! 

Reasons for geostatistics: description, explanation and control.

Description:
- classical surveys: data are summarized by means, medians, modes, variances, skewness, cummulative frequency distribution and histograms, box-plots. 
- geostatistical surveys: similarly + SPATIAL CORRELATION
This is represented by the EXPERIMENTAL OR SAMPLE VARIOGRAM. 
In a variogram the variance is estimated at increasing intervals of distance and several directions. Alternatively, it may be the corresponding set of spatial covariances or autocorrelation coefficients. 
In addition, we should plot the sampling points on a map ("posting"). This will show the extent to which the sample fills the region of interest, any clustering, and any obvious mistakes in recording the positions such as reversed coordinates. 

Interpretation:
- we draw the variogram, we fit the model. 
- we interpret them. Show how the properties change with distance (variograms for different directions). 
- interpret in a spatial sense.   

Control:
- the idea of controlling a process is often central in time-series analyses (also plagued by autocorrelation - temporal autocorrelation this time). 
- the results of the analysis are used to change the process itself (classic control) or decisions (where you cannot control the process as soil properties cannot be altered, they are given).

History of geostatistics:
- roots in mining, petroleum extraction, agronomy and forestry. 
Nowadays we find it in many fields from petroleun engineering, hydrogeology, meteorology, soil science, agriculture, pollution, fisheries to environmental protection. 

_______________________________________________

Chapter 2: BASIC STATISTICS

- basic quantitative methods for obtaining and summarizing information on the environment: the choice of variables and how to sample the environment.

- how these records can be used for estimation, prediction and mapping.
- in general  numerical variables and categorical variables (called "indicators" in geostat.)

ANOTHER FEATURE OF ENVIRONMENTAL DATA IS THAT THEY HAVE SPATIAL AND TEMPORAL COMPONENTS AS WELL AS RECORDED VALUES !
-> we distinguish MEASUREMENT, LOCATION and TIME. 

On the notation:
-x = {x1, x2} is a vector of the two spatial coordiantes
- Z(x) , bold x, means a random variable Z at place x,
- z(x) is the actual value of Z at place x.
- Greek letters for parameter notation and their ^ ("hat" versions) for their estimates.
  
   
Spatial aspects: 
- check how the variables behave at least in the directions of the axes of the coordinate system 
-> express spatial variation 
- use stratified sampling and see how the within-stratum variance is compared to the total variance.IF THERE IS SPATIAL DEPENDENCE -> WITHIN-STRATUM VARIANCE IS LESS THAN THAT OF A SIMPLE RANDOM SAMPLE FOR THE SAME EFFORT! (see more on p. 33). 
- use systematic sampling - for a more even cover use two-dimensional grids.
The problem is that we eliminate randomization, once we chose a starting point. So, the whole discussion on variance and standard error using tables falls out of discussion. An approximation may be obtained by dividing the region into strata and computing the pooled within-stratum variance as if sampling were random within the strata. The result will almost certainly be an overestimate (thus, conservative).

@bear pop. size estimation: find out shapes of the density stata, then randomly peak hunting units within strata. At the end multiply each stratum mean by a weight (the prop. of area from the total area) and see what you get for a mean computed based on these stratum means. For the variance, also compute the pooled within-stratum variance (W.&O.p. 34). You can get estimates of the within-class variance also my means of ANOVA. PS: You should actually use sampling units of equal sizes not hunting units. so,.. 

Alternatives: Yate's method of balanced differences.
Estimates of error by balanced differences are computed as follows:
- consider sampling on a transect (i.e. systematic sampling in one direction).  
- the transect is viewed through a small window containing, say, m sampling points with values z_1, z_2, ....., z_m. Then, we compute for the window the differences:

d_m = 1/2 z_1 - z_2 + z_3 - z_4 + ... + 1/2 z_m

We then move the window along the transect in steps and compute d_m at each new position (moving window analysis). The halves of z_1 and z_m are because you start each new window at the last z of the precedent window. But, I guess we can perform it by using the middle value of the current window as starting point for the next window. Anyway, for the above eq. the variance for the transect mean is the sum s^2(balanced differences) = (1/(J(m-2+0.5)))*sum(d^2) (p. 34). J is the number of steps of the window, and (m-2+0.5) is the sum of squares of the coefficients in the eq. on d_m.    

For a two-dimensional grid the window is squared. You carefully assign the coeff. of each cell and apply the s^2(balanced differences) eq. 
THIS KIND OF WINDOWS IN ONE AND TWO DIMENSIONS FILTER OUT THE LONG-RANGE FLUCTUATION, JUST AS STRATIFICATION. (because it acts like a local variance so to speak).
Garrard's moving window analyses really help a lot!

Soil classification: use the classes to stratify the region of interest!

IN GENERAL USE CERTAIN ATTRIBUTES TO STRATIFY A REGION OF INTEREST! (this is how I've got the idea for @bear population size estimation, I've used the bear density as a criterion) 

- you obtain REGIONAL MEAN: if the classification is good, then the poolen within-stratum variance of the region is smaller than the total variance. Classification should improve the precision or efficiency in estimating the regional mean.
- the mean mu is estimated as sum(w_k*z_meank). the w_k = (area of stratum k)/ (total area). z_meank is the estimated mean for kth stratum.  
- the estimated variance is: see p. 35.

THE AVERAGE WITHIN-CLASS VARIANCE CAN BE ESTIMATED FROM DATA BY ANOVA. IT CAN ALSO SERVE FOR PREDICTION. see next ch.
      

Chapter 3: PREDICTION AND INTERPOLATION

- spatial interpolation: measurements on the environment constitute a sample from a continuum that cannot be recorded everywhere. But we would like to have values also in the area in the intervening space: predict in a spatial sense. We also want a map of the spatial distribution of the variables => GEOSTATISTICAL PREDICTION CALLED KRIGING.

!!! NEARLY ALL THE METHODS OF PREDICTION, INCLUDING THE SIMPLER FORMS OF KRIGING, CAN BE SEEN AS WEIGHTED AVERAGES OF DATA !!!

-> such weighted averages look like Andrew's neural networks.
an estimated value at a target point x_0 is: the sum of all values measured weighted by lambda (each value has its own weight): z(x_0) = sum(lambda_i*z(x_i)).
          
How are these weights assigned?
- Thiessen polygons (Voronoi polygons, Dirichlet tessellation): 
lambda_i= 1 if x_i included in Vi (its tile) OR ELSE lambda_i = 0.

- Triangulation: 
determines the value lambda_1 from the apices of the triangle formed by {x_11, x_12}, {x_21, x_22} and {x_31, x_32}  and those of the target point {x_01, x_02} by linear interpolation. It applies the same procedure to lambda_2 and lambda_3.

@alternative: if you have a target point P1 for which you want to estimate the value for a variable you can use the 3 nearest neighbors P2, P3, and P4.
 The nearest neighbors are represented by the 3 neighbors which have the smallest distance to the target point, where the distance between P1 and P2 is given by d= sqrt( (x_2 - x_1)^2 + (y_2 - y_1)^2), i.e. Pythagora or the magnitude from M. Lewis videos, and the same for P1 and P3, and P1 and P4. 
If we add these distances to a total, we can find out the weight each neighbor receives in estimating the value for P1 by subtracting the result of dividing each distance to the total distance from 1, i.e. (1-(d_P1P2/d_total)).
 Also the local variance can give us a measure of how sure we are about the prediction. I have to test this idea on a DEM to see if I have a grid of elevation values and compute the nearest neighbors how well does it fit to the data.   



- Natural neighbor interpolation:
    
---------------------------------------------------------------

"Applied Geostatistics" Edward H. Isaaks, R. Mohan Srivastava

- ideas for an applied mathematics tutorial needed in environmental sciences:
		- basic calculus: derivatives and integrals
		- linear algebra: vectors and matrices
		- basic statistical concepts: univariate description, bivariate description (ch 2, and ch 3 in Isaaks and Srivastava).


Ch.1 Introduction 

The hallmark of a good geostatistical study is customization of the approach to the problem at hand (also Garrard 2016). 

The methods discussed are generally applicable to any data set in which the values are spatially continuous.

The used example data set uses the continuous variables: V and U (e.g. thickness of a geologic horizon or the conc. of some pollutant, rainfall measurememnts, or diameters of trees) . The discrete variable T can be a classification (binary one) (e.g. two different colors, two different species, two different rock types, presence or absence of somehting).

The example data set consists of V, U and T measurements at each of 78,000 points on a 1x1 m^2 grid. From this dataset a subset of 470 sample points has been chosen to represent the typical data set. 

Goals of the example case studies:
Using the 470 samples in the sample data set
- description of the important features of the data
- estiamtion of an average value over a large area
- estimation of an unknown value at a particular location
- estimation of an average value over small areas
- use of the available sampling to check the performance of an estimation methodology
- use of sample values of one variable to improve the estimation of another variable
- estimation of a distribution of values over a large area
- estimation of a distribution of values ober small areas
- estimation of a distribution of block averages
- the assessment of the uncertainty of our various estimates

Questions: 
- Do we want an estimate over a large area or estimates for specific local areas?
- Are we interested in some average value or in the complete distribution of values?
- Do we want to refer to some area of the same size as our sample data? Or to a different sized area?

ch. 2 - Univariate description

- it is assumed the material is already familiar to the reader.
- very good references at the end of the chapter. and a good refresher of univariate summary statistics. 

SHORT DIGRESSION FROM B. D. RIPLEY ("Spatial Statistics" 1981)
- no replication in spatial statistics
- patterns can be heterogeneous vs. homogeneous
- clustering vs. inhibiting factors
- patterns can also exhibit preferred directions (anisotropy) vs. isotropy
- data should be subdivided in sufficiently small units before invoking homogeneity or isotropy.
- a stoichastic process is stationary under translation if its distribution is unchanged the origin of the index set is translated.     
(in this case we can consider processes that are stationaty under rotations about the origin - isotropic processes). Homogeneous and isotropic processes are stationary under rigid motions.       
- covariances and spectra: covariance C and correlation R between Z(s) and Z(t) for two points in T are defined by: 
	C(s,t) = E[{Z(s) - E(Z(s))}*{Z(t) - E(Z(t))}]
	R(s,t) = C(s,t) / sqrt{C(s,s)*C(t,t)}
HOMOGENEITY IMPLIES THAT C AND R DEPEND ONLY ON d(s,t) WITH ISOTROPY!
IN GENERAL THE DISTRIBUTION OF A STOCHASTIC PROCESS IS NOT COMPLETELY DETERMINED BY THE MEAN m(s) = E[Z(s)] and covariance C(s,t)!!! This is the case for an important class of processes, the GAUSSIAN PROCESSES defined by the property that all finite collections {Z(t1), ..., Z(t_n)} are joint Normal. It is important to know which covariance functions can occur, for given m and C so that we can construct a Gaussian process via the Daniell-Kolmogorov theorem with that mean and variance. Good to know there are other types of stochastic processes not just the Gaussian: Poisson, Gibbs, Markow processes.

Back to Ed and Mo:
Ch. 3 : Bivariate Description
- also a good refresher.

Ch.4 : Spatial Description

One of the things that distinguishes earth science data sets from most others is that the data belong to some location in space.
- none of the univariate and bivariate tools from ch 2 and ch 3 capture spatial characteristics of the data set (location of extreme values, overall trend, degree of continuity).
- look at spatial characteristics and incorpotate them into the variables descriptions!

Data postings: 
A map on which each data location is plotted along with its corresponding data value. A data posting gives clues how the data were collected. Blank areas on the map may have been inaccessible, heavily sampled areas indicate some initial interest. 

Contour Maps:
The overall trends in the data values can be revealed by a contour map. Contouring by hand is an excellent way to become fami1iar with a data set.           
- it is performed with an interpolation algorithm 
The closeness of the contour lines indicates a steep gradient and draws our attention to the fact that the highest data value (145 ppm) is very close to the lowest data value (0 ppm).

Symbol Maps:


----------------------------------------------
Found an newer reference by Tomislav Hengl "A Practical Guide to Geostatistical Mapping" 2009.
- a lot of stuff in R but also a good theoretical introduction

- a very interesting reference: Cressie 1993.

---------------------------------------------- 

"Statistics for Spatial Data" Noel A.C. Cressie (1993)

- this really seams to be the ultimative book.

Consider an initial probability distribution p' equiv. to (p_1, ...., p_k).
This probability distribution evolves into a subsequent probability distribution p'A, where A is a kxk matrix of nonnegative elements each of whose rows sums to 1. A=(a_ij).Then a_ij can be interpreted as the conditional probability that the subsequent state of nature is j given that the initial state of nature was i. A is called a transition matrix (P. Dawkins). WHEN STATES OF NATURE EVOLVE THROUGH REPEATED APPLICATION OF A STOCHASTIC MECHANISM DESCRIBED BY THE MATRIX A, THE RESULT IS CALLED A MARKOV CHAIN. Variation increases through the chain. So, we cannot get around it, we have to consider it.

Independent-and-Identically Distributed - Data model (IID data) 
- assumes observations on a phenomenon are taken uder identical conditions and that each observation is taken independently of any other. 
- more often, configuration changes => not the same conditions => not quite IDD data sampled.

Inhomogeneous - Data model
- inhomogeneous data is accounted for in models by a NON-CONSTANT MEAN ASSUMPTION. So, the large-scale variations are somehow taken into account. But, there are also often reasons to suspect inhomogeneous small-scale variations. 
- the means might be constant but the variances differ between two experiments (phenomenon called heteroskedascity).
=> RELAXATION OF THE IDENTICAL-DISTRIBUTION ASSUMPTION.

RELAXATION OF THE INDEPENDENCE ASSUMPTION SI A FURTHER WAY TO GENERALIZE STATISTICAL MODELS => dependent-data model

Dependent - data model
- two classes of models: involve interclass-correlation structures and serial-correlation structures.
In spatial analysis dependence is present in all directions and becomes weaker as data locations become more dispersed. 

Purely temporal models, or time series models, are usually based on indetically distributed observations that are dependent and occur at equally spaced time points. The unidirectional flow of time underlies the construction of these models. 

Different fields (among them forestry, or any discipline that works with data collected from different spatial locations) need to develop (not necessarily statistical) models that indicate when there is dependence between measurements at different locations. 
However the models need to be more flexible than their temporal counterparts, because it is not reasonable to assume that spatial locations of data occur reguraly. 

DEPARTURES FROM THE INDEPENDENCE PARADIGM CAN BE MODELED. OR ROBUST PROCEDURES CAN BE CONSTRUCTED.

Crassie adopts the modeling approach.

Introductory example: rainfall
- has both the TEMPORAL AND SPATIAL COMPONENT!    
THE FUTURE OF SPATIAL ANALYSIS IS TO SOLVE PROBLEMS FOR SPATIOTEMPORAL DATA.
STATISTICS FOR SPATIAL AND TEMPORAL DATA WOULD PROVIDE DYNAMIC MODELS FOR THE PHENOMENA DISTRIBUTED THROUGH SPATE AND EVOLVING IN TIME!

TYPICAL EXAMPLE: DATA FROM A MONITORING NETWORK.
-one must take account of temporal correlations as well as spatial correlations             
- 26 rainfall recording stations in S. Australia.
- data are the monthly rainfall amounts at 26 locations over 30 years. 
- -> 1248 monthly observations, 26 spatial obs.
- over time and space they form a collection of approx. 20,000 observations.

Scope of the Crassie 1993: data analysis and statistical modeling of spatial data. THE BASIC COMPONENTS ARE SPATIAL LOCATIONS {s_1, ...., s_n} AND DATA {Z(s_1), ...., Z(s_n)}, OBSERVED AT THOSE LOCATIONS. Usually the data are assumed random and sometimes the locations are assumed random. GIS TOOLS FACILITATE THE INTEGRATION OF SPATIAL, NONSPATIAL, QUALITATIVE AND QUANTITATIVE DATA INTO A DATA BASE THAT CAN BE MANAGED UNDER ONE SYSTEM ENVIRONMENT.  


Also the distance between two known locations is the Euclidean distance, i.e. d=sqrt(sum((s_i - u_i)^2)), if the two points s and u are expressed as vectors. (which is the magnitude from M. Lewis videos).

Also clustering (which regions are considered close on a map according to some metric?) is considered through multidimensional scaling (important spatial contiguities - unmittelbare Nachbarschaft, might be violated) . 

SPATIAL ANALYSIS:
- GEOSTATISTICS -> DEALS WITH SPATIAL PROCESSES INDEXED OVER CONTINUOUS SPACE
- SPATIAL MODELS ON LATTICES ->
- SPATIAL POINT PATTERN ANALYSIS ->          
